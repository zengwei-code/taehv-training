#!/usr/bin/env python3
"""
TAEHV ç®€æ´æ¨ç†è„šæœ¬ - åªè´Ÿè´£æ¨ç†åŠŸèƒ½
ä¸“æ³¨äºè§†é¢‘ç¼–ç å’Œè§£ç ï¼Œä¸åŒ…å«è¯„ä¼°å’Œå¯è§†åŒ–
"""

import argparse
import time
from pathlib import Path
from typing import Optional, Tuple

import torch
import numpy as np
from tqdm import tqdm

from models.taehv import TAEHV
from training.dataset import MiniDataset


class SimpleInference:
    """ç®€æ´çš„TAEHVæ¨ç†å™¨ - å•ä¸€èŒè´£ï¼šæ¨ç†"""
    
    def __init__(self, model_path: str, device: str = "cuda"):
        self.device = device
        self.model = self._load_model(model_path)
        self.model.eval()
        
    def _load_model(self, model_path: str) -> TAEHV:
        """åŠ è½½æ¨¡å‹"""
        print(f"Loading model from {model_path}")
        model = TAEHV(checkpoint_path=None)
        
        state_dict = torch.load(model_path, map_location=self.device, weights_only=True)
        if 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']
        
        model.load_state_dict(state_dict)
        model = model.to(self.device)
        
        param_count = sum(p.numel() for p in model.parameters()) / 1e6
        print(f"âœ… Model loaded: {param_count:.2f}M parameters")
        return model
    
    @torch.no_grad()
    def encode(self, video: torch.Tensor, parallel: bool = True) -> torch.Tensor:
        """
        ç¼–ç è§†é¢‘åˆ°æ½œåœ¨ç©ºé—´
        Args:
            video: [N,T,C,H,W] è§†é¢‘å¼ é‡ï¼ŒèŒƒå›´ [0,1]
            parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†
        Returns:
            latent: [N,T',C',H',W'] æ½œåœ¨è¡¨ç¤º
        """
        video = video.to(self.device)
        latent = self.model.encode_video(video, parallel=parallel, show_progress_bar=False)
        return latent
    
    @torch.no_grad()
    def decode(self, latent: torch.Tensor, parallel: bool = True) -> torch.Tensor:
        """
        ä»æ½œåœ¨ç©ºé—´è§£ç è§†é¢‘
        Args:
            latent: [N,T,C,H,W] æ½œåœ¨è¡¨ç¤º
            parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†
        Returns:
            video: [N,T',C',H',W'] é‡å»ºè§†é¢‘ï¼ŒèŒƒå›´ [0,1]
        """
        latent = latent.to(self.device)
        video = self.model.decode_video(latent, parallel=parallel, show_progress_bar=False)
        return video
    
    @torch.no_grad()
    def reconstruct(self, video: torch.Tensor, parallel: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å®Œæ•´çš„é‡å»ºæµç¨‹ï¼šç¼–ç  -> è§£ç 
        Args:
            video: [N,T,C,H,W] è¾“å…¥è§†é¢‘
            parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†
        Returns:
            reconstructed: é‡å»ºçš„è§†é¢‘
            latent: æ½œåœ¨è¡¨ç¤º
        """
        latent = self.encode(video, parallel=parallel)
        reconstructed = self.decode(latent, parallel=parallel)
        return reconstructed, latent
    
    def benchmark(self, video: torch.Tensor, num_runs: int = 10) -> dict:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        Args:
            video: æµ‹è¯•è§†é¢‘
            num_runs: è¿è¡Œæ¬¡æ•°
        Returns:
            æ€§èƒ½ç»Ÿè®¡å­—å…¸
        """
        video = video.to(self.device)
        
        # é¢„çƒ­
        _ = self.reconstruct(video)
        
        encode_times = []
        decode_times = []
        
        for _ in range(num_runs):
            # ç¼–ç 
            torch.cuda.synchronize() if self.device == 'cuda' else None
            start = time.time()
            latent = self.encode(video)
            torch.cuda.synchronize() if self.device == 'cuda' else None
            encode_times.append(time.time() - start)
            
            # è§£ç 
            torch.cuda.synchronize() if self.device == 'cuda' else None
            start = time.time()
            _ = self.decode(latent)
            torch.cuda.synchronize() if self.device == 'cuda' else None
            decode_times.append(time.time() - start)
        
        return {
            'encode_mean': np.mean(encode_times),
            'encode_std': np.std(encode_times),
            'decode_mean': np.mean(decode_times),
            'decode_std': np.std(decode_times),
            'total_mean': np.mean(encode_times) + np.mean(decode_times),
        }


def main():
    parser = argparse.ArgumentParser(description="TAEHV Simple Inference")
    parser.add_argument("--model_path", type=str, required=True, help="Path to model checkpoint")
    parser.add_argument("--data_root", type=str, default="/data/matrix-project/MiniDataset/data")
    parser.add_argument("--annotation_file", type=str, 
                       default="/data/matrix-project/MiniDataset/stage1_annotations_500.json")
    parser.add_argument("--num_samples", type=int, default=5, help="Number of samples")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--save_latents", action="store_true", help="Save latent representations")
    parser.add_argument("--output_dir", type=str, default="inference_output")
    parser.add_argument("--benchmark", action="store_true", help="Run performance benchmark")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = SimpleInference(args.model_path, args.device)
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ Loading dataset...")
    dataset = MiniDataset(
        annotation_file=args.annotation_file,
        data_dir=args.data_root,
        patch_hw=128,
        n_frames=12
    )
    print(f"âœ… Loaded {len(dataset)} samples")
    
    # æ¨ç†
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸš€ Running inference on {args.num_samples} samples...")
    
    all_latents = []
    all_reconstructions = []
    all_originals = []
    
    for i in tqdm(range(min(args.num_samples, len(dataset)))):
        video = dataset[i].unsqueeze(0).float().to(args.device)
        
        # æ¨ç†
        reconstructed, latent = inferencer.reconstruct(video)
        
        # å¯¹é½å¸§æ•°
        if inferencer.model.frames_to_trim > 0:
            video_aligned = video[:, inferencer.model.frames_to_trim:]
        else:
            video_aligned = video
        
        if args.save_latents:
            all_latents.append(latent.cpu())
        all_reconstructions.append(reconstructed.cpu())
        all_originals.append(video_aligned.cpu())
    
    # ä¿å­˜ç»“æœ
    if args.save_latents:
        latents_path = output_dir / "latents.pt"
        torch.save(torch.cat(all_latents, dim=0), latents_path)
        print(f"âœ… Latents saved to {latents_path}")
    
    recon_path = output_dir / "reconstructions.pt"
    torch.save(torch.cat(all_reconstructions, dim=0), recon_path)
    print(f"âœ… Reconstructions saved to {recon_path}")
    
    orig_path = output_dir / "originals.pt"
    torch.save(torch.cat(all_originals, dim=0), orig_path)
    print(f"âœ… Originals saved to {orig_path}")
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    if args.benchmark:
        print("\nâ±ï¸  Running performance benchmark...")
        test_video = dataset[0].unsqueeze(0).float()
        stats = inferencer.benchmark(test_video, num_runs=20)
        print(f"\nğŸ“Š Performance Results:")
        print(f"   Encoding: {stats['encode_mean']*1000:.2f} Â± {stats['encode_std']*1000:.2f} ms")
        print(f"   Decoding: {stats['decode_mean']*1000:.2f} Â± {stats['decode_std']*1000:.2f} ms")
        print(f"   Total:    {stats['total_mean']*1000:.2f} ms")
    
    print("\nâœ… Inference completed!")


if __name__ == "__main__":
    main()

