#!/usr/bin/env python3
"""
TAEHV å¯è§†åŒ–å·¥å…· - ä¸“æ³¨äºç»“æœå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import argparse
from pathlib import Path
from typing import Optional

import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image


class TAEHVVisualizer:
    """TAEHV ç»“æœå¯è§†åŒ–å™¨"""
    
    @staticmethod
    def save_video_tensor(video_tensor: torch.Tensor, output_path: Path, fps: int = 8):
        """
        ä¿å­˜è§†é¢‘å¼ é‡ä¸ºMP4æ–‡ä»¶
        Args:
            video_tensor: [T,C,H,W] è§†é¢‘å¼ é‡ï¼ŒèŒƒå›´ [0,1]
            output_path: è¾“å‡ºè·¯å¾„
            fps: å¸§ç‡
        """
        frames = []
        for t in range(video_tensor.shape[0]):
            frame = video_tensor[t].cpu().numpy().transpose(1, 2, 0)
            frame = np.clip(frame * 255, 0, 255).astype(np.uint8)
            frame = np.ascontiguousarray(frame)  # ç¡®ä¿è¿ç»­å†…å­˜å¸ƒå±€
            frames.append(frame)
        
        if len(frames) == 0:
            return
        
        height, width = frames[0].shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
        
        for frame in frames:
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            out.write(frame_bgr)
        out.release()
        print(f"âœ… Video saved to {output_path}")
    
    @staticmethod
    def create_comparison_video(
        original: torch.Tensor, 
        reconstructed: torch.Tensor, 
        output_path: Path, 
        fps: int = 8
    ):
        """
        åˆ›å»ºå·¦å³å¯¹æ¯”è§†é¢‘
        Args:
            original: [T,C,H,W] åŸå§‹è§†é¢‘
            reconstructed: [T,C,H,W] é‡å»ºè§†é¢‘
            output_path: è¾“å‡ºè·¯å¾„
            fps: å¸§ç‡
        """
        frames = []
        for t in range(original.shape[0]):
            orig_frame = original[t].cpu().numpy().transpose(1, 2, 0)
            recon_frame = reconstructed[t].cpu().numpy().transpose(1, 2, 0)
            
            orig_frame = np.clip(orig_frame * 255, 0, 255).astype(np.uint8)
            recon_frame = np.clip(recon_frame * 255, 0, 255).astype(np.uint8)
            
            # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„ï¼ˆOpenCVè¦æ±‚ï¼‰
            orig_frame = np.ascontiguousarray(orig_frame)
            recon_frame = np.ascontiguousarray(recon_frame)
            
            # æ·»åŠ æ ‡ç­¾
            cv2.putText(orig_frame, "Original", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(recon_frame, "Reconstructed", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            comparison = np.hstack([orig_frame, recon_frame])
            frames.append(comparison)
        
        if len(frames) == 0:
            return
        
        height, width = frames[0].shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
        
        for frame in frames:
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            out.write(frame_bgr)
        out.release()
        print(f"âœ… Comparison video saved to {output_path}")
    
    @staticmethod
    def visualize_latent(
        latent: torch.Tensor, 
        output_path: Path,
        max_channels: int = 16
    ):
        """
        å¯è§†åŒ–æ½œåœ¨è¡¨ç¤º
        Args:
            latent: [T,C,H,W] æ½œåœ¨å¼ é‡
            output_path: è¾“å‡ºè·¯å¾„
            max_channels: æœ€å¤šæ˜¾ç¤ºçš„é€šé“æ•°
        """
        T, C, H, W = latent.shape
        n_channels = min(C, max_channels)
        
        # é€‰æ‹©ä¸­é—´å¸§
        mid_t = T // 2
        latent_frame = latent[mid_t].cpu().numpy()
        
        # åˆ›å»ºç½‘æ ¼
        cols = 4
        rows = (n_channels + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))
        axes = axes.flatten() if n_channels > 1 else [axes]
        
        for i in range(n_channels):
            channel = latent_frame[i]
            # å½’ä¸€åŒ–åˆ° [0, 1]
            channel = (channel - channel.min()) / (channel.max() - channel.min() + 1e-8)
            
            axes[i].imshow(channel, cmap='viridis')
            axes[i].set_title(f'Channel {i}')
            axes[i].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_channels, len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… Latent visualization saved to {output_path}")
    
    @staticmethod
    def create_error_heatmap(
        original: torch.Tensor,
        reconstructed: torch.Tensor,
        output_path: Path,
        frame_idx: int = 0
    ):
        """
        åˆ›å»ºè¯¯å·®çƒ­å›¾
        Args:
            original: [T,C,H,W] åŸå§‹è§†é¢‘
            reconstructed: [T,C,H,W] é‡å»ºè§†é¢‘
            output_path: è¾“å‡ºè·¯å¾„
            frame_idx: è¦å¯è§†åŒ–çš„å¸§ç´¢å¼•
        """
        orig_frame = original[frame_idx].cpu().numpy().transpose(1, 2, 0)
        recon_frame = reconstructed[frame_idx].cpu().numpy().transpose(1, 2, 0)
        
        # è®¡ç®—è¯¯å·®
        error = np.abs(orig_frame - recon_frame).mean(axis=2)
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸå§‹å¸§
        axes[0].imshow(np.clip(orig_frame, 0, 1))
        axes[0].set_title('Original', fontsize=14, fontweight='bold')
        axes[0].axis('off')
        
        # é‡å»ºå¸§
        axes[1].imshow(np.clip(recon_frame, 0, 1))
        axes[1].set_title('Reconstructed', fontsize=14, fontweight='bold')
        axes[1].axis('off')
        
        # è¯¯å·®çƒ­å›¾
        im = axes[2].imshow(error, cmap='hot', vmin=0, vmax=0.2)
        axes[2].set_title('Error Heatmap', fontsize=14, fontweight='bold')
        axes[2].axis('off')
        plt.colorbar(im, ax=axes[2], fraction=0.046)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… Error heatmap saved to {output_path}")
    
    @staticmethod
    def create_summary_grid(
        originals: torch.Tensor,
        reconstructions: torch.Tensor,
        output_path: Path,
        num_samples: int = 9
    ):
        """
        åˆ›å»ºæ ·æœ¬ç½‘æ ¼å¯¹æ¯”å›¾
        Args:
            originals: [N,T,C,H,W] åŸå§‹è§†é¢‘æ‰¹æ¬¡
            reconstructions: [N,T,C,H,W] é‡å»ºè§†é¢‘æ‰¹æ¬¡
            output_path: è¾“å‡ºè·¯å¾„
            num_samples: æ˜¾ç¤ºæ ·æœ¬æ•°
        """
        N = min(originals.shape[0], num_samples)
        cols = 3
        rows = (N + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols*2, figsize=(cols*6, rows*3))
        axes = axes.flatten() if N > 1 else [axes]
        
        for i in range(N):
            # å–æ¯ä¸ªè§†é¢‘çš„é¦–å¸§
            orig = originals[i, 0].cpu().numpy().transpose(1, 2, 0)
            recon = reconstructions[i, 0].cpu().numpy().transpose(1, 2, 0)
            
            # åŸå§‹å¸§
            axes[i*2].imshow(np.clip(orig, 0, 1))
            axes[i*2].set_title(f'Sample {i} - Original')
            axes[i*2].axis('off')
            
            # é‡å»ºå¸§
            axes[i*2+1].imshow(np.clip(recon, 0, 1))
            axes[i*2+1].set_title(f'Sample {i} - Recon')
            axes[i*2+1].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(N*2, len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… Summary grid saved to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="TAEHV Visualization Tool")
    parser.add_argument("--originals", type=str, required=True, 
                       help="Path to originals.pt")
    parser.add_argument("--reconstructions", type=str, required=True, 
                       help="Path to reconstructions.pt")
    parser.add_argument("--latents", type=str, 
                       help="Path to latents.pt (optional)")
    parser.add_argument("--output_dir", type=str, default="visualizations")
    parser.add_argument("--sample_idx", type=int, default=0,
                       help="Sample index for detailed visualization")
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ Loading data...")
    originals = torch.load(args.originals, weights_only=False)
    reconstructions = torch.load(args.reconstructions, weights_only=False)
    print(f"âœ… Loaded {originals.shape[0]} samples")
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    visualizer = TAEHVVisualizer()
    
    # 1. åˆ›å»ºå¯¹æ¯”è§†é¢‘
    print("\nğŸ¬ Creating comparison videos...")
    visualizer.create_comparison_video(
        originals[args.sample_idx],
        reconstructions[args.sample_idx],
        output_dir / f"sample_{args.sample_idx:03d}_comparison.mp4"
    )
    
    # 2. åˆ›å»ºè¯¯å·®çƒ­å›¾
    print("\nğŸ”¥ Creating error heatmap...")
    visualizer.create_error_heatmap(
        originals[args.sample_idx],
        reconstructions[args.sample_idx],
        output_dir / f"sample_{args.sample_idx:03d}_error.png",
        frame_idx=0
    )
    
    # 3. åˆ›å»ºæ ·æœ¬ç½‘æ ¼
    print("\nğŸ–¼ï¸  Creating summary grid...")
    visualizer.create_summary_grid(
        originals,
        reconstructions,
        output_dir / "summary_grid.png",
        num_samples=min(9, originals.shape[0])
    )
    
    # 4. å¯è§†åŒ–æ½œåœ¨è¡¨ç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
    if args.latents:
        print("\nğŸ”® Visualizing latent space...")
        latents = torch.load(args.latents, weights_only=False)
        visualizer.visualize_latent(
            latents[args.sample_idx],
            output_dir / f"sample_{args.sample_idx:03d}_latent.png"
        )
    
    print(f"\nâœ… All visualizations saved to {output_dir}")


if __name__ == "__main__":
    main()

