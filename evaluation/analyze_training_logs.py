"""
Training Logs Analysis Script
åˆ†æTensorBoardè®­ç»ƒæ—¥å¿—ï¼Œè¯„ä¼°è®­ç»ƒè¿‡ç¨‹è´¨é‡

è¯„ä¼°å†…å®¹ï¼š
1. æŸå¤±æ›²çº¿è¶‹åŠ¿
2. æ”¶æ•›æ€§åˆ†æ
3. è¿‡æ‹Ÿåˆæ£€æµ‹
4. æœ€ä½³checkpointè¯†åˆ«
"""

import tensorflow as tf
from tensorboard.backend.event_processing import event_accumulator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from scipy import stats
import argparse
import json


class TrainingAnalyzer:
    """è®­ç»ƒæ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_dir):
        self.log_dir = Path(log_dir)
        self.metrics_data = {}
        
    def load_tensorboard_logs(self):
        """åŠ è½½TensorBoardæ—¥å¿—"""
        print(f"ğŸ“‚ Loading logs from: {self.log_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰eventæ–‡ä»¶
        event_files = list(self.log_dir.glob('events.out.tfevents.*'))
        
        if not event_files:
            print(f"âŒ No event files found in {self.log_dir}")
            return False
        
        print(f"âœ… Found {len(event_files)} event files")
        
        # åˆå¹¶æ‰€æœ‰eventæ–‡ä»¶çš„æ•°æ®
        all_scalars = {}
        
        for event_file in event_files:
            try:
                ea = event_accumulator.EventAccumulator(str(event_file))
                ea.Reload()
                
                # è·å–æ‰€æœ‰æ ‡é‡æŒ‡æ ‡
                scalar_tags = ea.Tags()['scalars']
                
                for tag in scalar_tags:
                    events = ea.Scalars(tag)
                    
                    if tag not in all_scalars:
                        all_scalars[tag] = {'steps': [], 'values': [], 'wall_time': []}
                    
                    for event in events:
                        all_scalars[tag]['steps'].append(event.step)
                        all_scalars[tag]['values'].append(event.value)
                        all_scalars[tag]['wall_time'].append(event.wall_time)
                
                print(f"   Loaded: {event_file.name}")
                
            except Exception as e:
                print(f"âš ï¸  Error loading {event_file.name}: {e}")
                continue
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶æ’åº
        for tag in all_scalars:
            # æŒ‰stepæ’åº
            sorted_indices = np.argsort(all_scalars[tag]['steps'])
            all_scalars[tag]['steps'] = np.array(all_scalars[tag]['steps'])[sorted_indices]
            all_scalars[tag]['values'] = np.array(all_scalars[tag]['values'])[sorted_indices]
            all_scalars[tag]['wall_time'] = np.array(all_scalars[tag]['wall_time'])[sorted_indices]
        
        self.metrics_data = all_scalars
        
        print(f"\nâœ… Loaded metrics: {list(all_scalars.keys())}")
        
        return True
    
    def analyze_convergence(self, metric_name, window_size=100):
        """åˆ†ææ”¶æ•›æ€§"""
        if metric_name not in self.metrics_data:
            return None
        
        values = self.metrics_data[metric_name]['values']
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        moving_avg = pd.Series(values).rolling(window=window_size, min_periods=1).mean()
        
        # è®¡ç®—è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰
        if len(values) > window_size:
            recent_values = values[-window_size:]
            x = np.arange(len(recent_values))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, recent_values)
            
            is_converged = abs(slope) < 0.0001  # æ–œç‡æ¥è¿‘0
            trend = 'decreasing' if slope < 0 else 'increasing' if slope > 0 else 'stable'
        else:
            slope = 0
            is_converged = False
            trend = 'insufficient_data'
        
        # è®¡ç®—å˜å¼‚ç³»æ•° (æ ‡å‡†å·®/å‡å€¼)
        recent_std = np.std(values[-window_size:]) if len(values) > window_size else np.std(values)
        recent_mean = np.mean(values[-window_size:]) if len(values) > window_size else np.mean(values)
        cv = recent_std / (recent_mean + 1e-8)
        
        return {
            'moving_average': moving_avg.values.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            'trend': trend,
            'slope': float(slope),
            'is_converged': bool(is_converged),  # è½¬æ¢ä¸ºPython bool
            'coefficient_of_variation': float(cv),
            'final_value': float(values[-1]),
            'best_value': float(np.min(values)) if 'loss' in metric_name.lower() else float(np.max(values)),
            'mean_value': float(np.mean(values)),
            'std_value': float(np.std(values)),
        }
    
    def detect_overfitting(self, train_metric, val_metric):
        """æ£€æµ‹è¿‡æ‹Ÿåˆ"""
        if train_metric not in self.metrics_data or val_metric not in self.metrics_data:
            print(f"âš ï¸  Cannot detect overfitting: missing {train_metric} or {val_metric}")
            return None
        
        train_values = self.metrics_data[train_metric]['values']
        val_values = self.metrics_data[val_metric]['values']
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´ï¼ˆå–è¾ƒçŸ­çš„ï¼‰
        min_len = min(len(train_values), len(val_values))
        train_values = train_values[:min_len]
        val_values = val_values[:min_len]
        
        # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å·®è·
        gap = np.abs(train_values - val_values)
        
        # æ£€æµ‹gapæ˜¯å¦åœ¨å¢å¤§
        if len(gap) > 100:
            early_gap = np.mean(gap[:len(gap)//2])
            late_gap = np.mean(gap[len(gap)//2:])
            gap_increase = late_gap - early_gap
            
            is_overfitting = gap_increase > early_gap * 0.2  # gapå¢åŠ è¶…è¿‡20%
        else:
            is_overfitting = False
            gap_increase = 0
        
        return {
            'is_overfitting': is_overfitting,
            'gap_increase': float(gap_increase),
            'final_gap': float(gap[-1]),
            'mean_gap': float(np.mean(gap)),
        }
    
    def find_best_checkpoint(self, metric_name, mode='min'):
        """æ‰¾åˆ°æœ€ä½³checkpoint"""
        if metric_name not in self.metrics_data:
            return None
        
        values = self.metrics_data[metric_name]['values']
        steps = self.metrics_data[metric_name]['steps']
        
        if mode == 'min':
            best_idx = np.argmin(values)
        else:
            best_idx = np.argmax(values)
        
        return {
            'best_step': int(steps[best_idx]),
            'best_value': float(values[best_idx]),
            'metric': metric_name,
        }
    
    def plot_training_curves(self, save_dir='evaluation_results'):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ†ç»„ç»˜åˆ¶ä¸åŒç±»å‹çš„æŒ‡æ ‡
        loss_metrics = [k for k in self.metrics_data.keys() if 'loss' in k.lower()]
        other_metrics = [k for k in self.metrics_data.keys() if 'loss' not in k.lower()]
        
        # 1. ç»˜åˆ¶Lossæ›²çº¿
        if loss_metrics:
            n_loss = len(loss_metrics)
            fig, axes = plt.subplots((n_loss + 1) // 2, 2, figsize=(15, 5 * ((n_loss + 1) // 2)))
            if n_loss == 1:
                axes = np.array([axes])
            axes = axes.flatten()
            
            for idx, metric in enumerate(loss_metrics):
                steps = self.metrics_data[metric]['steps']
                values = self.metrics_data[metric]['values']
                
                axes[idx].plot(steps, values, linewidth=1.5, alpha=0.8, label=metric)
                
                # æ·»åŠ ç§»åŠ¨å¹³å‡
                analysis = self.analyze_convergence(metric)
                if analysis:
                    axes[idx].plot(steps, analysis['moving_average'], 
                                 linewidth=2, alpha=0.6, linestyle='--', 
                                 label=f'Moving Avg (trend: {analysis["trend"]})')
                
                axes[idx].set_xlabel('Training Steps')
                axes[idx].set_ylabel('Loss')
                axes[idx].set_title(metric)
                axes[idx].legend()
                axes[idx].grid(True, alpha=0.3)
            
            # éšè—å¤šä½™çš„å­å›¾
            for idx in range(len(loss_metrics), len(axes)):
                axes[idx].axis('off')
            
            plt.tight_layout()
            plt.savefig(save_dir / 'training_losses.png', dpi=150, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“Š Loss curves saved to: {save_dir / 'training_losses.png'}")
        
        # 2. ç»˜åˆ¶å…¶ä»–æŒ‡æ ‡
        if other_metrics:
            n_metrics = len(other_metrics)
            fig, axes = plt.subplots((n_metrics + 1) // 2, 2, figsize=(15, 5 * ((n_metrics + 1) // 2)))
            if n_metrics == 1:
                axes = np.array([axes])
            axes = axes.flatten()
            
            for idx, metric in enumerate(other_metrics):
                steps = self.metrics_data[metric]['steps']
                values = self.metrics_data[metric]['values']
                
                axes[idx].plot(steps, values, linewidth=1.5, alpha=0.8)
                axes[idx].set_xlabel('Training Steps')
                axes[idx].set_ylabel('Value')
                axes[idx].set_title(metric)
                axes[idx].grid(True, alpha=0.3)
            
            # éšè—å¤šä½™çš„å­å›¾
            for idx in range(len(other_metrics), len(axes)):
                axes[idx].axis('off')
            
            plt.tight_layout()
            plt.savefig(save_dir / 'training_metrics.png', dpi=150, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“Š Metric curves saved to: {save_dir / 'training_metrics.png'}")
    
    def generate_report(self, save_path='evaluation_results/training_analysis.json'):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š Training Analysis Report")
        print("="*60)
        
        report = {
            'metrics': {},
            'convergence': {},
            'best_checkpoints': {},
            'recommendations': []
        }
        
        # åˆ†ææ¯ä¸ªæŒ‡æ ‡
        for metric_name in self.metrics_data.keys():
            analysis = self.analyze_convergence(metric_name)
            
            if analysis:
                report['metrics'][metric_name] = analysis
                
                print(f"\nğŸ“ˆ {metric_name}")
                print(f"   Trend: {analysis['trend']}")
                print(f"   Converged: {'Yes' if analysis['is_converged'] else 'No'}")
                print(f"   Final Value: {analysis['final_value']:.6f}")
                print(f"   Best Value: {analysis['best_value']:.6f}")
                print(f"   Mean Â± Std: {analysis['mean_value']:.6f} Â± {analysis['std_value']:.6f}")
                print(f"   Variation Coef: {analysis['coefficient_of_variation']:.4f}")
        
        # æ‰¾åˆ°æœ€ä½³checkpoint
        loss_metrics = [k for k in self.metrics_data.keys() if 'loss' in k.lower() and 'train' in k.lower()]
        
        if loss_metrics:
            main_loss = loss_metrics[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè®­ç»ƒlossä½œä¸ºä¸»è¦æŒ‡æ ‡
            best_ckpt = self.find_best_checkpoint(main_loss, mode='min')
            
            if best_ckpt:
                report['best_checkpoints']['by_loss'] = best_ckpt
                print(f"\nğŸ† Best Checkpoint (by {main_loss})")
                print(f"   Step: {best_ckpt['best_step']}")
                print(f"   Value: {best_ckpt['best_value']:.6f}")
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(report)
        report['recommendations'] = recommendations
        
        print("\n" + "="*60)
        print("ğŸ’¡ Recommendations")
        print("="*60)
        for rec in recommendations:
            print(f"   â€¢ {rec}")
        
        # ä¿å­˜æŠ¥å‘Š
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(save_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nğŸ’¾ Report saved to: {save_path}")
        print("="*60)
        
        return report
    
    def _generate_recommendations(self, report):
        """ç”Ÿæˆè®­ç»ƒå»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥æ˜¯å¦æ”¶æ•›
        loss_metrics = [k for k in report['metrics'].keys() if 'loss' in k.lower()]
        
        if loss_metrics:
            main_loss = loss_metrics[0]
            analysis = report['metrics'][main_loss]
            
            if not analysis['is_converged']:
                if analysis['trend'] == 'decreasing':
                    recommendations.append(
                        "Loss is still decreasing. Consider training for more steps."
                    )
                elif analysis['trend'] == 'increasing':
                    recommendations.append(
                        "âš ï¸  Loss is increasing! Check for training instability or overfitting."
                    )
            else:
                recommendations.append(
                    "âœ… Training has converged. Model is ready for evaluation."
                )
            
            # æ£€æŸ¥å˜å¼‚ç³»æ•°
            if analysis['coefficient_of_variation'] > 0.1:
                recommendations.append(
                    "Training loss has high variation. Consider reducing learning rate or using gradient clipping."
                )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰validationæŒ‡æ ‡
        val_metrics = [k for k in report['metrics'].keys() if 'val' in k.lower() or 'validation' in k.lower()]
        
        if not val_metrics:
            recommendations.append(
                "âš ï¸  No validation metrics found. Add validation to monitor overfitting."
            )
        
        return recommendations


def main():
    parser = argparse.ArgumentParser(description='Analyze Training Logs')
    parser.add_argument('--log_dir', type=str, required=True, 
                       help='Path to TensorBoard log directory')
    parser.add_argument('--output_dir', type=str, default='./evaluation_results',
                       help='Directory to save analysis results')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = TrainingAnalyzer(args.log_dir)
    
    # åŠ è½½æ—¥å¿—
    if not analyzer.load_tensorboard_logs():
        print("âŒ Failed to load logs")
        return
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    analyzer.plot_training_curves(save_dir=args.output_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_report(save_path=f'{args.output_dir}/training_analysis.json')


if __name__ == '__main__':
    main()

