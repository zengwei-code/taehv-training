"""
Quick Evaluation Script - Python Version
å¿«é€Ÿè¯„ä¼°è„šæœ¬ - Pythonç‰ˆæœ¬

ä¸€é”®è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. è®­ç»ƒæ—¥å¿—åˆ†æ
2. æ¨¡å‹å®šé‡è¯„ä¼°
3. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python quick_evaluate.py --model_path xxx --data_root xxx --annotation_file xxx
"""

import subprocess
import argparse
import json
import os
from pathlib import Path
import sys

# ä¿®å¤GLIBCXXç‰ˆæœ¬é—®é¢˜ï¼ˆmatplotlibä¾èµ–ï¼‰
os.environ['LD_LIBRARY_PATH'] = '/data1/anaconda3/envs/tiny-vae/lib:' + os.environ.get('LD_LIBRARY_PATH', '')


def print_header(text):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"ğŸ¯ {text}")
    print("=" * 60)


def print_success(text):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"âœ… {text}")


def print_warning(text):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    print(f"âš ï¸  {text}")


def print_error(text):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"âŒ {text}")


def detect_log_dir(project_root):
    """è‡ªåŠ¨æ£€æµ‹æ—¥å¿—ç›®å½•"""
    logs_dir = project_root / 'logs'
    if not logs_dir.exists():
        return None
    
    # æŸ¥æ‰¾åŒ…å«æœ€å¤šäº‹ä»¶æ–‡ä»¶çš„æ—¥å¿—ç›®å½•
    log_candidates = []
    for subdir in logs_dir.iterdir():
        if subdir.is_dir():
            event_files = list(subdir.glob('events.out.tfevents.*'))
            if event_files:
                log_candidates.append((subdir, len(event_files)))
    
    if log_candidates:
        # è¿”å›åŒ…å«æœ€å¤šäº‹ä»¶æ–‡ä»¶çš„ç›®å½•
        log_candidates.sort(key=lambda x: x[1], reverse=True)
        return str(log_candidates[0][0])
    
    return None


def validate_paths(args, project_root):
    """éªŒè¯å’Œä¿®æ­£è·¯å¾„å‚æ•°"""
    issues = []
    suggestions = []
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not Path(args.model_path).exists():
        issues.append(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        
        # å°è¯•åœ¨outputç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
        output_dir = project_root / 'output'
        if output_dir.exists():
            model_candidates = []
            for subdir in sorted(output_dir.iterdir(), reverse=True):
                if subdir.is_dir():
                    best_model = subdir / 'best_model' / 'model.pth'
                    final_model = subdir / 'final_model.pth'
                    if best_model.exists():
                        model_candidates.append(str(best_model))
                        break
                    elif final_model.exists():
                        model_candidates.append(str(final_model))
                        break
            
            if model_candidates:
                suggestions.append(f"å»ºè®®ä½¿ç”¨: --model_path {model_candidates[0]}")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not Path(args.config).exists():
        issues.append(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        
        # å°è¯•æŸ¥æ‰¾å…¶ä»–é…ç½®æ–‡ä»¶
        config_dir = project_root / 'training' / 'configs'
        if config_dir.exists():
            config_files = list(config_dir.glob('taehv_config_*.py'))
            if config_files:
                suggestions.append(f"å»ºè®®ä½¿ç”¨: --config {config_files[0]}")
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„ï¼ˆæœ€é‡è¦ï¼‰
    if not args.data_root or not Path(args.data_root).exists():
        issues.append("æ•°æ®é›†æ ¹ç›®å½•æœªæŒ‡å®šæˆ–ä¸å­˜åœ¨")
        suggestions.append("å¿…é¡»ä½¿ç”¨: --data_root /path/to/dataset")
        suggestions.append("ç¤ºä¾‹: --data_root /data/matrix-project/MiniDataset/data")
    
    if not args.annotation_file or not Path(args.annotation_file).exists():
        issues.append("æ ‡æ³¨æ–‡ä»¶æœªæŒ‡å®šæˆ–ä¸å­˜åœ¨")
        suggestions.append("å¿…é¡»ä½¿ç”¨: --annotation_file /path/to/annotations.json")
        suggestions.append("ç¤ºä¾‹: --annotation_file /data/matrix-project/MiniDataset/stage1_annotations_500.json")
    
    # æ£€æŸ¥æ—¥å¿—ç›®å½•ï¼ˆå¯é€‰ï¼‰
    if args.log_dir and not Path(args.log_dir).exists():
        detected_log = detect_log_dir(project_root)
        if detected_log:
            issues.append(f"æŒ‡å®šçš„æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {args.log_dir}")
            suggestions.append(f"æ£€æµ‹åˆ°æ—¥å¿—ç›®å½•: --log_dir {detected_log}")
            args.log_dir = detected_log  # è‡ªåŠ¨ä¿®æ­£
        else:
            issues.append("æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—ç›®å½•ï¼Œå°†è·³è¿‡æ—¥å¿—åˆ†æ")
    
    return issues, suggestions


def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    print(f"\nâ³ {description}...")
    try:
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=False,
            text=True
        )
        return True
    except subprocess.CalledProcessError as e:
        print_error(f"{description}å¤±è´¥")
        return False
    except FileNotFoundError:
        print_error(f"å‘½ä»¤æœªæ‰¾åˆ°: {cmd[0]}")
        return False


def analyze_training_logs(log_dir, output_dir, script_dir):
    """åˆ†æè®­ç»ƒæ—¥å¿—"""
    print_header("æ­¥éª¤1/3: åˆ†æè®­ç»ƒæ—¥å¿—")
    
    if not Path(log_dir).exists():
        print_warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}ï¼Œè·³è¿‡")
        return False
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„è°ƒç”¨è„šæœ¬
    analyze_script = script_dir / "analyze_training_logs.py"
    
    cmd = [
        "python", str(analyze_script),
        "--log_dir", log_dir,
        "--output_dir", output_dir
    ]
    
    success = run_command(cmd, "è®­ç»ƒæ—¥å¿—åˆ†æ")
    
    if success:
        print_success("è®­ç»ƒæ—¥å¿—åˆ†æå®Œæˆ")
        print(f"  - æŸå¤±æ›²çº¿: {output_dir}/training_losses.png")
        print(f"  - æŒ‡æ ‡æ›²çº¿: {output_dir}/training_metrics.png")
        print(f"  - åˆ†ææŠ¥å‘Š: {output_dir}/training_analysis.json")
    
    return success


def evaluate_model(model_path, config, num_samples, batch_size, output_dir, script_dir, data_root=None, annotation_file=None):
    """è¯„ä¼°æ¨¡å‹"""
    print_header("æ­¥éª¤2/3: æ¨¡å‹å®šé‡è¯„ä¼°")
    
    if not Path(model_path).exists():
        print_error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    if not Path(config).exists():
        print_error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config}")
        return False
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„è°ƒç”¨è„šæœ¬
    evaluate_script = script_dir / "evaluate_vae.py"
    
    cmd = [
        "python", str(evaluate_script),
        "--model_path", model_path,
        "--config", config,
        "--num_samples", str(num_samples),
        "--batch_size", str(batch_size)
    ]
    
    # æ·»åŠ å¯é€‰çš„æ•°æ®é›†å‚æ•°
    if data_root:
        cmd.extend(["--data_root", data_root])
    if annotation_file:
        cmd.extend(["--annotation_file", annotation_file])
    
    success = run_command(cmd, "æ¨¡å‹è¯„ä¼°")
    
    if success:
        print_success("æ¨¡å‹è¯„ä¼°å®Œæˆ")
        print(f"  - è¯„ä¼°ç»“æœ: {output_dir}/evaluation_results.json")
        print(f"  - æŒ‡æ ‡åˆ†å¸ƒ: {output_dir}/metrics_distribution.png")
        print(f"  - å¯è§†åŒ–æ ·æœ¬: {output_dir}/sample_*.png")
    
    return success


def generate_summary(output_dir):
    """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
    print_header("æ­¥éª¤3/3: ç”Ÿæˆè¯„ä¼°æ‘˜è¦")
    
    try:
        # è¯»å–è¯„ä¼°ç»“æœ
        eval_results_path = Path(output_dir) / "evaluation_results.json"
        if not eval_results_path.exists():
            print_warning("è¯„ä¼°ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        with open(eval_results_path, 'r') as f:
            eval_results = json.load(f)
        
        print("\n" + "=" * 60)
        print("ğŸ¯ è¯„ä¼°æ‘˜è¦")
        print("=" * 60)
        
        # æå–å…³é”®æŒ‡æ ‡
        psnr = eval_results['psnr']['mean']
        ssim = eval_results['ssim']['mean']
        lpips = eval_results['lpips']['mean']
        
        print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"  PSNR:  {psnr:.2f} dB")
        print(f"  SSIM:  {ssim:.4f}")
        print(f"  LPIPS: {lpips:.4f}")
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        psnr_score = min(psnr / 35.0, 1.0) * 100
        ssim_score = ssim * 100
        lpips_score = max(1.0 - lpips * 10, 0.0) * 100
        overall_score = (psnr_score * 0.3 + ssim_score * 0.4 + lpips_score * 0.3)
        
        print(f"\nğŸ† ç»¼åˆè¯„åˆ†: {overall_score:.1f}/100")
        
        # è´¨é‡è¯„çº§
        if overall_score >= 85:
            quality = "ğŸŒŸ Excellent (ä¼˜ç§€)"
        elif overall_score >= 70:
            quality = "âœ… Good (è‰¯å¥½)"
        elif overall_score >= 55:
            quality = "âš ï¸  Fair (ä¸€èˆ¬)"
        else:
            quality = "âŒ Poor (è¾ƒå·®)"
        
        print(f"  è´¨é‡ç­‰çº§: {quality}")
        
        # è¯»å–è®­ç»ƒåˆ†æï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        training_analysis_path = Path(output_dir) / "training_analysis.json"
        if training_analysis_path.exists():
            with open(training_analysis_path, 'r') as f:
                training_analysis = json.load(f)
            
            print("\nğŸ“ˆ è®­ç»ƒçŠ¶æ€:")
            
            # æŸ¥æ‰¾ä¸»è¦lossæŒ‡æ ‡
            loss_metrics = [k for k in training_analysis['metrics'].keys() 
                          if 'loss' in k.lower()]
            if loss_metrics:
                main_loss = loss_metrics[0]
                loss_info = training_analysis['metrics'][main_loss]
                
                print(f"  è¶‹åŠ¿: {loss_info['trend']}")
                print(f"  æ”¶æ•›: {'æ˜¯' if loss_info['is_converged'] else 'å¦'}")
                print(f"  æœ€ç»ˆå€¼: {loss_info['final_value']:.6f}")
            
            # æ˜¾ç¤ºå»ºè®®
            if training_analysis.get('recommendations'):
                print("\nğŸ’¡ å»ºè®®:")
                for rec in training_analysis['recommendations'][:3]:
                    print(f"  â€¢ {rec}")
        
        print("\n" + "=" * 60)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_path = Path(output_dir) / "EVALUATION_REPORT.md"
        generate_markdown_report(eval_results, training_analysis if training_analysis_path.exists() else None, 
                                report_path, overall_score, quality)
        print_success(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return True
        
    except Exception as e:
        print_error(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_markdown_report(eval_results, training_analysis, save_path, overall_score, quality):
    """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ“Š VAEæ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {Path.cwd()}\n\n")
        
        f.write("## ğŸ¯ ç»¼åˆè¯„ä¼°\n\n")
        f.write(f"- **ç»¼åˆè¯„åˆ†**: {overall_score:.1f}/100\n")
        f.write(f"- **è´¨é‡ç­‰çº§**: {quality}\n\n")
        
        f.write("## ğŸ“Š å®šé‡æŒ‡æ ‡\n\n")
        f.write("| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ |\n")
        f.write("|------|------|--------|--------|--------|\n")
        
        for metric_name, stats in eval_results.items():
            f.write(f"| {metric_name.upper()} | {stats['mean']:.4f} | "
                   f"{stats['std']:.4f} | {stats['min']:.4f} | {stats['max']:.4f} |\n")
        
        f.write("\n### ğŸ“ˆ æŒ‡æ ‡è§£è¯»\n\n")
        
        psnr = eval_results['psnr']['mean']
        ssim = eval_results['ssim']['mean']
        lpips = eval_results['lpips']['mean']
        
        f.write(f"- **PSNR = {psnr:.2f} dB**\n")
        if psnr > 35:
            f.write("  - âœ… ä¼˜ç§€ - é‡å»ºè´¨é‡éå¸¸é«˜\n")
        elif psnr > 30:
            f.write("  - âœ… è‰¯å¥½ - é‡å»ºè´¨é‡å¯æ¥å—\n")
        elif psnr > 25:
            f.write("  - âš ï¸  ä¸€èˆ¬ - æœ‰æ˜æ˜¾å¤±çœŸ\n")
        else:
            f.write("  - âŒ è¾ƒå·® - å¤±çœŸä¸¥é‡\n")
        
        f.write(f"\n- **SSIM = {ssim:.4f}**\n")
        if ssim > 0.95:
            f.write("  - âœ… ä¼˜ç§€ - ç»“æ„ä¿ç•™å®Œç¾\n")
        elif ssim > 0.90:
            f.write("  - âœ… è‰¯å¥½ - ç»“æ„ä¿ç•™è¾ƒå¥½\n")
        elif ssim > 0.85:
            f.write("  - âš ï¸  ä¸€èˆ¬ - æœ‰ç»“æ„æŸå¤±\n")
        else:
            f.write("  - âŒ è¾ƒå·® - ç»“æ„å˜åŒ–æ˜æ˜¾\n")
        
        f.write(f"\n- **LPIPS = {lpips:.4f}**\n")
        if lpips < 0.05:
            f.write("  - âœ… ä¼˜ç§€ - æ„ŸçŸ¥è´¨é‡æé«˜\n")
        elif lpips < 0.10:
            f.write("  - âœ… è‰¯å¥½ - æ„ŸçŸ¥è´¨é‡è¾ƒå¥½\n")
        elif lpips < 0.20:
            f.write("  - âš ï¸  ä¸€èˆ¬ - æ„ŸçŸ¥å·®å¼‚å¯å¯Ÿè§‰\n")
        else:
            f.write("  - âŒ è¾ƒå·® - æ„ŸçŸ¥è´¨é‡ä¸ä½³\n")
        
        # è®­ç»ƒåˆ†æ
        if training_analysis:
            f.write("\n## ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹åˆ†æ\n\n")
            
            loss_metrics = [k for k in training_analysis['metrics'].keys() 
                          if 'loss' in k.lower()]
            
            if loss_metrics:
                f.write("### LossæŒ‡æ ‡\n\n")
                for loss_name in loss_metrics:
                    loss_info = training_analysis['metrics'][loss_name]
                    f.write(f"**{loss_name}**:\n")
                    f.write(f"- è¶‹åŠ¿: {loss_info['trend']}\n")
                    f.write(f"- æ˜¯å¦æ”¶æ•›: {'æ˜¯' if loss_info['is_converged'] else 'å¦'}\n")
                    f.write(f"- æœ€ç»ˆå€¼: {loss_info['final_value']:.6f}\n")
                    f.write(f"- æœ€ä½³å€¼: {loss_info['best_value']:.6f}\n\n")
            
            if training_analysis.get('recommendations'):
                f.write("### ğŸ’¡ å»ºè®®\n\n")
                for rec in training_analysis['recommendations']:
                    f.write(f"- {rec}\n")
        
        f.write("\n## ğŸ“ æ–‡ä»¶æ¸…å•\n\n")
        f.write("è¯„ä¼°è¿‡ç¨‹ç”Ÿæˆäº†ä»¥ä¸‹æ–‡ä»¶ï¼š\n\n")
        f.write("- `evaluation_results.json` - è¯¦ç»†è¯„ä¼°æ•°æ®\n")
        f.write("- `metrics_distribution.png` - æŒ‡æ ‡åˆ†å¸ƒå›¾\n")
        f.write("- `sample_*.png` - å¯è§†åŒ–é‡å»ºæ ·æœ¬\n")
        f.write("- `training_losses.png` - è®­ç»ƒæŸå¤±æ›²çº¿\n")
        f.write("- `training_metrics.png` - è®­ç»ƒæŒ‡æ ‡æ›²çº¿\n")
        f.write("- `training_analysis.json` - è®­ç»ƒè¿‡ç¨‹åˆ†æ\n")
        
        f.write("\n---\n\n")
        f.write("*æŠ¥å‘Šç”± `quick_evaluate.py` è‡ªåŠ¨ç”Ÿæˆ*\n")


def main():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
    script_dir = Path(__file__).parent  # evaluation/
    project_root = script_dir.parent    # my_taehv_training/
    
    # æ™ºèƒ½æ£€æµ‹é»˜è®¤è·¯å¾„
    detected_log_dir = detect_log_dir(project_root)
    default_log_dir = detected_log_dir or str(project_root / 'logs')
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
    default_model = None
    output_dir = project_root / 'output'
    if output_dir.exists():
        for subdir in sorted(output_dir.iterdir(), reverse=True):
            if subdir.is_dir():
                best_model = subdir / 'best_model' / 'model.pth'
                if best_model.exists():
                    default_model = str(best_model)
                    break
    
    parser = argparse.ArgumentParser(
        description='å¿«é€Ÿè¯„ä¼°VAEæ¨¡å‹ - ä¸€é”®è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®Œæ•´ç¤ºä¾‹:
  python evaluation/quick_evaluate.py \\
      --model_path output/a800_2025-10-14_12-11-50/best_model/model.pth \\
      --data_root /data/matrix-project/MiniDataset/data \\
      --annotation_file /data/matrix-project/MiniDataset/stage1_annotations_500.json \\
      --num_samples 100

å¿«é€Ÿç¤ºä¾‹ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ï¼‰:
  python evaluation/quick_evaluate.py \\
      --data_root /data/matrix-project/MiniDataset/data \\
      --annotation_file /data/matrix-project/MiniDataset/stage1_annotations_500.json
        """
    )
    
    # å¿…éœ€å‚æ•°ç»„
    required_group = parser.add_argument_group('å¿…éœ€å‚æ•°ï¼ˆæ¨èæ˜ç¡®æŒ‡å®šï¼‰')
    required_group.add_argument(
        '--data_root',
        type=str,
        required=False,
        help='â­ æ•°æ®é›†æ ¹ç›®å½• [å¿…éœ€] - ç¤ºä¾‹: /data/matrix-project/MiniDataset/data'
    )
    
    required_group.add_argument(
        '--annotation_file',
        type=str,
        required=False,
        help='â­ æ ‡æ³¨æ–‡ä»¶è·¯å¾„ [å¿…éœ€] - ç¤ºä¾‹: /data/matrix-project/MiniDataset/stage1_annotations_500.json'
    )
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    model_group = parser.add_argument_group('æ¨¡å‹ç›¸å…³å‚æ•°')
    model_group.add_argument(
        '--model_path',
        type=str,
        default=default_model,
        help=f'æ¨¡å‹checkpointè·¯å¾„ (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹æœ€æ–°æ¨¡å‹)'
    )
    
    model_group.add_argument(
        '--config',
        type=str,
        default=str(project_root / 'training' / 'configs' / 'taehv_config_a800.py'),
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: training/configs/taehv_config_a800.py)'
    )
    
    # è¯„ä¼°å‚æ•°
    eval_group = parser.add_argument_group('è¯„ä¼°å‚æ•°')
    eval_group.add_argument(
        '--num_samples',
        type=int,
        default=100,
        help='è¯„ä¼°æ ·æœ¬æ•°é‡ (é»˜è®¤: 100)'
    )
    
    eval_group.add_argument(
        '--batch_size',
        type=int,
        default=4,
        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 4)'
    )
    
    # æ—¥å¿—å’Œè¾“å‡º
    log_group = parser.add_argument_group('æ—¥å¿—å’Œè¾“å‡º')
    log_group.add_argument(
        '--log_dir',
        type=str,
        default=default_log_dir,
        help=f'è®­ç»ƒæ—¥å¿—ç›®å½• (é»˜è®¤: {default_log_dir})'
    )
    
    log_group.add_argument(
        '--output_dir',
        type=str,
        default=str(script_dir / 'evaluation_results'),
        help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: evaluation/evaluation_results)'
    )
    
    log_group.add_argument(
        '--skip_logs',
        action='store_true',
        help='è·³è¿‡è®­ç»ƒæ—¥å¿—åˆ†æ'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è·¯å¾„
    issues, suggestions = validate_paths(args, project_root)
    
    # æ‰“å°é…ç½®
    print_header("ğŸš€ å¿«é€Ÿè¯„ä¼° Tiny-VAE æ¨¡å‹")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ—¥å¿—ç›®å½•: {args.log_dir}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {args.num_samples}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"ç»“æœç›®å½•: {args.output_dir}")
    if args.data_root:
        print(f"æ•°æ®é›†æ ¹ç›®å½•: {args.data_root}")
    if args.annotation_file:
        print(f"Annotationæ–‡ä»¶: {args.annotation_file}")
    
    # æ˜¾ç¤ºé—®é¢˜å’Œå»ºè®®
    if issues:
        print("\n" + "="*60)
        print("âš ï¸  å‘ç°ä»¥ä¸‹é—®é¢˜:")
        print("="*60)
        for issue in issues:
            print(f"  â€¢ {issue}")
        
        if suggestions:
            print("\nğŸ’¡ å»ºè®®:")
            for suggestion in suggestions:
                print(f"  â€¢ {suggestion}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è‡´å‘½é”™è¯¯ï¼ˆæ•°æ®é›†è·¯å¾„ï¼‰
        if not args.data_root or not Path(args.data_root).exists() or \
           not args.annotation_file or not Path(args.annotation_file).exists():
            print("\n" + "="*60)
            print_error("æ•°æ®é›†è·¯å¾„æ˜¯å¿…éœ€çš„ï¼Œæ— æ³•ç»§ç»­è¯„ä¼°")
            print("="*60)
            print("\nè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œ:")
            print("  python evaluation/quick_evaluate.py \\")
            print("      --data_root /data/matrix-project/MiniDataset/data \\")
            print("      --annotation_file /data/matrix-project/MiniDataset/stage1_annotations_500.json \\")
            if default_model:
                print(f"      --model_path {default_model} \\")
            print("      --num_samples 100")
            print()
            sys.exit(1)
        
        print("\nå°†å°è¯•ç»§ç»­æ‰§è¡Œ...")
        print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # æ­¥éª¤1: åˆ†æè®­ç»ƒæ—¥å¿—
    if not args.skip_logs:
        analyze_training_logs(args.log_dir, args.output_dir, script_dir)
    else:
        print_warning("è·³è¿‡è®­ç»ƒæ—¥å¿—åˆ†æï¼ˆ--skip_logsï¼‰")
    
    # æ­¥éª¤2: è¯„ä¼°æ¨¡å‹
    if not evaluate_model(args.model_path, args.config, args.num_samples, args.batch_size,
                         args.output_dir, script_dir,
                         data_root=args.data_root, annotation_file=args.annotation_file):
        print_error("æ¨¡å‹è¯„ä¼°å¤±è´¥ï¼Œç»ˆæ­¢")
        sys.exit(1)
    
    # æ­¥éª¤3: ç”Ÿæˆæ‘˜è¦
    generate_summary(args.output_dir)
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print_success("æ‰€æœ‰è¯„ä¼°æ­¥éª¤å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“‚ ç»“æœæ–‡ä»¶ä½ç½®: {args.output_dir}/")
    print(f"\nğŸ“„ è¯¦ç»†æ•°æ®: {args.output_dir}/evaluation_results.json")
    print(f"\nğŸ“– æŸ¥çœ‹ä½¿ç”¨æŒ‡å—: cat README.md")
    print(f"ğŸ“– æŸ¥çœ‹æ•…éšœæ’é™¤: cat TROUBLESHOOTING.md")
    print(f"\nğŸ¨ æŸ¥çœ‹å¯è§†åŒ–ç»“æœ:")
    print(f"   - {args.output_dir}/sample_1.png")
    print(f"   - {args.output_dir}/metrics_distribution.png")
    print(f"   - {args.output_dir}/training_losses.png")
    
    print(f"\nğŸ“Š å¯åŠ¨TensorBoardæŸ¥çœ‹è¯¦ç»†è®­ç»ƒè¿‡ç¨‹:")
    print(f"   tensorboard --logdir {args.log_dir} --port 6006")
    print()


if __name__ == '__main__':
    main()

