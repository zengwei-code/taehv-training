"""
Quick Evaluation Script - Python Version
å¿«é€Ÿè¯„ä¼°è„šæœ¬ - Pythonç‰ˆæœ¬

ä¸€é”®è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. è®­ç»ƒæ—¥å¿—åˆ†æ
2. æ¨¡å‹å®šé‡è¯„ä¼°
3. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python quick_evaluate.py
    python quick_evaluate.py --model_path output/xxx/final_model.pth --num_samples 200
"""

import subprocess
import argparse
import json
from pathlib import Path
import sys


def print_header(text):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"ğŸ¯ {text}")
    print("=" * 60)


def print_success(text):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"âœ… {text}")


def print_warning(text):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    print(f"âš ï¸  {text}")


def print_error(text):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"âŒ {text}")


def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    print(f"\nâ³ {description}...")
    try:
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=False,
            text=True
        )
        return True
    except subprocess.CalledProcessError as e:
        print_error(f"{description}å¤±è´¥")
        return False
    except FileNotFoundError:
        print_error(f"å‘½ä»¤æœªæ‰¾åˆ°: {cmd[0]}")
        return False


def analyze_training_logs(log_dir, output_dir):
    """åˆ†æè®­ç»ƒæ—¥å¿—"""
    print_header("æ­¥éª¤1/3: åˆ†æè®­ç»ƒæ—¥å¿—")
    
    if not Path(log_dir).exists():
        print_warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}ï¼Œè·³è¿‡")
        return False
    
    cmd = [
        "python", "analyze_training_logs.py",
        "--log_dir", log_dir,
        "--output_dir", output_dir
    ]
    
    success = run_command(cmd, "è®­ç»ƒæ—¥å¿—åˆ†æ")
    
    if success:
        print_success("è®­ç»ƒæ—¥å¿—åˆ†æå®Œæˆ")
        print(f"  - æŸå¤±æ›²çº¿: {output_dir}/training_losses.png")
        print(f"  - æŒ‡æ ‡æ›²çº¿: {output_dir}/training_metrics.png")
        print(f"  - åˆ†ææŠ¥å‘Š: {output_dir}/training_analysis.json")
    
    return success


def evaluate_model(model_path, config, num_samples, output_dir, data_root=None, annotation_file=None):
    """è¯„ä¼°æ¨¡å‹"""
    print_header("æ­¥éª¤2/3: æ¨¡å‹å®šé‡è¯„ä¼°")
    
    if not Path(model_path).exists():
        print_error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    if not Path(config).exists():
        print_error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config}")
        return False
    
    cmd = [
        "python", "evaluate_vae.py",
        "--model_path", model_path,
        "--config", config,
        "--num_samples", str(num_samples),
        "--batch_size", "4"
    ]
    
    # æ·»åŠ å¯é€‰çš„æ•°æ®é›†å‚æ•°
    if data_root:
        cmd.extend(["--data_root", data_root])
    if annotation_file:
        cmd.extend(["--annotation_file", annotation_file])
    
    success = run_command(cmd, "æ¨¡å‹è¯„ä¼°")
    
    if success:
        print_success("æ¨¡å‹è¯„ä¼°å®Œæˆ")
        print(f"  - è¯„ä¼°ç»“æœ: {output_dir}/evaluation_results.json")
        print(f"  - æŒ‡æ ‡åˆ†å¸ƒ: {output_dir}/metrics_distribution.png")
        print(f"  - å¯è§†åŒ–æ ·æœ¬: {output_dir}/sample_*.png")
    
    return success


def generate_summary(output_dir):
    """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
    print_header("æ­¥éª¤3/3: ç”Ÿæˆè¯„ä¼°æ‘˜è¦")
    
    try:
        # è¯»å–è¯„ä¼°ç»“æœ
        eval_results_path = Path(output_dir) / "evaluation_results.json"
        if not eval_results_path.exists():
            print_warning("è¯„ä¼°ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        with open(eval_results_path, 'r') as f:
            eval_results = json.load(f)
        
        print("\n" + "=" * 60)
        print("ğŸ¯ è¯„ä¼°æ‘˜è¦")
        print("=" * 60)
        
        # æå–å…³é”®æŒ‡æ ‡
        psnr = eval_results['psnr']['mean']
        ssim = eval_results['ssim']['mean']
        lpips = eval_results['lpips']['mean']
        
        print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"  PSNR:  {psnr:.2f} dB")
        print(f"  SSIM:  {ssim:.4f}")
        print(f"  LPIPS: {lpips:.4f}")
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        psnr_score = min(psnr / 35.0, 1.0) * 100
        ssim_score = ssim * 100
        lpips_score = max(1.0 - lpips * 10, 0.0) * 100
        overall_score = (psnr_score * 0.3 + ssim_score * 0.4 + lpips_score * 0.3)
        
        print(f"\nğŸ† ç»¼åˆè¯„åˆ†: {overall_score:.1f}/100")
        
        # è´¨é‡è¯„çº§
        if overall_score >= 85:
            quality = "ğŸŒŸ Excellent (ä¼˜ç§€)"
        elif overall_score >= 70:
            quality = "âœ… Good (è‰¯å¥½)"
        elif overall_score >= 55:
            quality = "âš ï¸  Fair (ä¸€èˆ¬)"
        else:
            quality = "âŒ Poor (è¾ƒå·®)"
        
        print(f"  è´¨é‡ç­‰çº§: {quality}")
        
        # è¯»å–è®­ç»ƒåˆ†æï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        training_analysis_path = Path(output_dir) / "training_analysis.json"
        if training_analysis_path.exists():
            with open(training_analysis_path, 'r') as f:
                training_analysis = json.load(f)
            
            print("\nğŸ“ˆ è®­ç»ƒçŠ¶æ€:")
            
            # æŸ¥æ‰¾ä¸»è¦lossæŒ‡æ ‡
            loss_metrics = [k for k in training_analysis['metrics'].keys() 
                          if 'loss' in k.lower()]
            if loss_metrics:
                main_loss = loss_metrics[0]
                loss_info = training_analysis['metrics'][main_loss]
                
                print(f"  è¶‹åŠ¿: {loss_info['trend']}")
                print(f"  æ”¶æ•›: {'æ˜¯' if loss_info['is_converged'] else 'å¦'}")
                print(f"  æœ€ç»ˆå€¼: {loss_info['final_value']:.6f}")
            
            # æ˜¾ç¤ºå»ºè®®
            if training_analysis.get('recommendations'):
                print("\nğŸ’¡ å»ºè®®:")
                for rec in training_analysis['recommendations'][:3]:
                    print(f"  â€¢ {rec}")
        
        print("\n" + "=" * 60)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_path = Path(output_dir) / "EVALUATION_REPORT.md"
        generate_markdown_report(eval_results, training_analysis if training_analysis_path.exists() else None, 
                                report_path, overall_score, quality)
        print_success(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return True
        
    except Exception as e:
        print_error(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_markdown_report(eval_results, training_analysis, save_path, overall_score, quality):
    """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ“Š VAEæ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {Path.cwd()}\n\n")
        
        f.write("## ğŸ¯ ç»¼åˆè¯„ä¼°\n\n")
        f.write(f"- **ç»¼åˆè¯„åˆ†**: {overall_score:.1f}/100\n")
        f.write(f"- **è´¨é‡ç­‰çº§**: {quality}\n\n")
        
        f.write("## ğŸ“Š å®šé‡æŒ‡æ ‡\n\n")
        f.write("| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ |\n")
        f.write("|------|------|--------|--------|--------|\n")
        
        for metric_name, stats in eval_results.items():
            f.write(f"| {metric_name.upper()} | {stats['mean']:.4f} | "
                   f"{stats['std']:.4f} | {stats['min']:.4f} | {stats['max']:.4f} |\n")
        
        f.write("\n### ğŸ“ˆ æŒ‡æ ‡è§£è¯»\n\n")
        
        psnr = eval_results['psnr']['mean']
        ssim = eval_results['ssim']['mean']
        lpips = eval_results['lpips']['mean']
        
        f.write(f"- **PSNR = {psnr:.2f} dB**\n")
        if psnr > 35:
            f.write("  - âœ… ä¼˜ç§€ - é‡å»ºè´¨é‡éå¸¸é«˜\n")
        elif psnr > 30:
            f.write("  - âœ… è‰¯å¥½ - é‡å»ºè´¨é‡å¯æ¥å—\n")
        elif psnr > 25:
            f.write("  - âš ï¸  ä¸€èˆ¬ - æœ‰æ˜æ˜¾å¤±çœŸ\n")
        else:
            f.write("  - âŒ è¾ƒå·® - å¤±çœŸä¸¥é‡\n")
        
        f.write(f"\n- **SSIM = {ssim:.4f}**\n")
        if ssim > 0.95:
            f.write("  - âœ… ä¼˜ç§€ - ç»“æ„ä¿ç•™å®Œç¾\n")
        elif ssim > 0.90:
            f.write("  - âœ… è‰¯å¥½ - ç»“æ„ä¿ç•™è¾ƒå¥½\n")
        elif ssim > 0.85:
            f.write("  - âš ï¸  ä¸€èˆ¬ - æœ‰ç»“æ„æŸå¤±\n")
        else:
            f.write("  - âŒ è¾ƒå·® - ç»“æ„å˜åŒ–æ˜æ˜¾\n")
        
        f.write(f"\n- **LPIPS = {lpips:.4f}**\n")
        if lpips < 0.05:
            f.write("  - âœ… ä¼˜ç§€ - æ„ŸçŸ¥è´¨é‡æé«˜\n")
        elif lpips < 0.10:
            f.write("  - âœ… è‰¯å¥½ - æ„ŸçŸ¥è´¨é‡è¾ƒå¥½\n")
        elif lpips < 0.20:
            f.write("  - âš ï¸  ä¸€èˆ¬ - æ„ŸçŸ¥å·®å¼‚å¯å¯Ÿè§‰\n")
        else:
            f.write("  - âŒ è¾ƒå·® - æ„ŸçŸ¥è´¨é‡ä¸ä½³\n")
        
        # è®­ç»ƒåˆ†æ
        if training_analysis:
            f.write("\n## ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹åˆ†æ\n\n")
            
            loss_metrics = [k for k in training_analysis['metrics'].keys() 
                          if 'loss' in k.lower()]
            
            if loss_metrics:
                f.write("### LossæŒ‡æ ‡\n\n")
                for loss_name in loss_metrics:
                    loss_info = training_analysis['metrics'][loss_name]
                    f.write(f"**{loss_name}**:\n")
                    f.write(f"- è¶‹åŠ¿: {loss_info['trend']}\n")
                    f.write(f"- æ˜¯å¦æ”¶æ•›: {'æ˜¯' if loss_info['is_converged'] else 'å¦'}\n")
                    f.write(f"- æœ€ç»ˆå€¼: {loss_info['final_value']:.6f}\n")
                    f.write(f"- æœ€ä½³å€¼: {loss_info['best_value']:.6f}\n\n")
            
            if training_analysis.get('recommendations'):
                f.write("### ğŸ’¡ å»ºè®®\n\n")
                for rec in training_analysis['recommendations']:
                    f.write(f"- {rec}\n")
        
        f.write("\n## ğŸ“ æ–‡ä»¶æ¸…å•\n\n")
        f.write("è¯„ä¼°è¿‡ç¨‹ç”Ÿæˆäº†ä»¥ä¸‹æ–‡ä»¶ï¼š\n\n")
        f.write("- `evaluation_results.json` - è¯¦ç»†è¯„ä¼°æ•°æ®\n")
        f.write("- `metrics_distribution.png` - æŒ‡æ ‡åˆ†å¸ƒå›¾\n")
        f.write("- `sample_*.png` - å¯è§†åŒ–é‡å»ºæ ·æœ¬\n")
        f.write("- `training_losses.png` - è®­ç»ƒæŸå¤±æ›²çº¿\n")
        f.write("- `training_metrics.png` - è®­ç»ƒæŒ‡æ ‡æ›²çº¿\n")
        f.write("- `training_analysis.json` - è®­ç»ƒè¿‡ç¨‹åˆ†æ\n")
        
        f.write("\n---\n\n")
        f.write("*æŠ¥å‘Šç”± `quick_evaluate.py` è‡ªåŠ¨ç”Ÿæˆ*\n")


def main():
    parser = argparse.ArgumentParser(
        description='å¿«é€Ÿè¯„ä¼°VAEæ¨¡å‹ - ä¸€é”®è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python quick_evaluate.py
  python quick_evaluate.py --model_path output/xxx/final_model.pth
  python quick_evaluate.py --num_samples 200
        """
    )
    
    parser.add_argument(
        '--model_path',
        type=str,
        default='../output/2025-10-01_19-59-50/final_model.pth',
        help='æ¨¡å‹checkpointè·¯å¾„ (é»˜è®¤: ../output/2025-10-01_19-59-50/final_model.pth)'
    )
    
    parser.add_argument(
        '--log_dir',
        type=str,
        default='../logs/taehv_h100_production',
        help='è®­ç»ƒæ—¥å¿—ç›®å½• (é»˜è®¤: ../logs/taehv_h100_production)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='../training/configs/taehv_config_h100.py',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ../training/configs/taehv_config_h100.py)'
    )
    
    parser.add_argument(
        '--num_samples',
        type=int,
        default=100,
        help='è¯„ä¼°æ ·æœ¬æ•°é‡ (é»˜è®¤: 100)'
    )
    
    parser.add_argument(
        '--output_dir',
        type=str,
        default='./evaluation_results',
        help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./evaluation_results)'
    )
    
    parser.add_argument(
        '--skip_logs',
        action='store_true',
        help='è·³è¿‡è®­ç»ƒæ—¥å¿—åˆ†æ'
    )
    
    parser.add_argument(
        '--data_root',
        type=str,
        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é›†æ ¹ç›®å½•'
    )
    
    parser.add_argument(
        '--annotation_file',
        type=str,
        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„annotationæ–‡ä»¶è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®
    print_header("ğŸš€ å¿«é€Ÿè¯„ä¼° Tiny-VAE æ¨¡å‹")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ—¥å¿—ç›®å½•: {args.log_dir}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {args.num_samples}")
    print(f"ç»“æœç›®å½•: {args.output_dir}")
    if args.data_root:
        print(f"æ•°æ®é›†æ ¹ç›®å½•: {args.data_root} (è¦†ç›–)")
    if args.annotation_file:
        print(f"Annotationæ–‡ä»¶: {args.annotation_file} (è¦†ç›–)")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # æ­¥éª¤1: åˆ†æè®­ç»ƒæ—¥å¿—
    if not args.skip_logs:
        analyze_training_logs(args.log_dir, args.output_dir)
    else:
        print_warning("è·³è¿‡è®­ç»ƒæ—¥å¿—åˆ†æï¼ˆ--skip_logsï¼‰")
    
    # æ­¥éª¤2: è¯„ä¼°æ¨¡å‹
    if not evaluate_model(args.model_path, args.config, args.num_samples, args.output_dir,
                         data_root=args.data_root, annotation_file=args.annotation_file):
        print_error("æ¨¡å‹è¯„ä¼°å¤±è´¥ï¼Œç»ˆæ­¢")
        sys.exit(1)
    
    # æ­¥éª¤3: ç”Ÿæˆæ‘˜è¦
    generate_summary(args.output_dir)
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print_success("æ‰€æœ‰è¯„ä¼°æ­¥éª¤å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“‚ ç»“æœæ–‡ä»¶ä½ç½®: {args.output_dir}/")
    print(f"\nğŸ“„ è¯¦ç»†æ•°æ®: {args.output_dir}/evaluation_results.json")
    print(f"\nğŸ“– æŸ¥çœ‹ä½¿ç”¨æŒ‡å—: cat README.md")
    print(f"ğŸ“– æŸ¥çœ‹æ•…éšœæ’é™¤: cat TROUBLESHOOTING.md")
    print(f"\nğŸ¨ æŸ¥çœ‹å¯è§†åŒ–ç»“æœ:")
    print(f"   - {args.output_dir}/sample_1.png")
    print(f"   - {args.output_dir}/metrics_distribution.png")
    print(f"   - {args.output_dir}/training_losses.png")
    
    print(f"\nğŸ“Š å¯åŠ¨TensorBoardæŸ¥çœ‹è¯¦ç»†è®­ç»ƒè¿‡ç¨‹:")
    print(f"   tensorboard --logdir {args.log_dir} --port 6006")
    print()


if __name__ == '__main__':
    main()

