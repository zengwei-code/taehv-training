"""
VAE Model Evaluation Script
ç§‘å­¦è¯„ä¼° Tiny-VAE æ¨¡å‹çš„è´¨é‡

è¯„ä¼°ç»´åº¦ï¼š
1. å®šé‡æŒ‡æ ‡ï¼šPSNR, SSIM, LPIPS, MSE
2. è§†è§‰è´¨é‡ï¼šé‡å»ºå¯¹æ¯”ã€è¯¯å·®çƒ­å›¾
3. æ½œåœ¨ç©ºé—´è´¨é‡ï¼šå‹ç¼©æ¯”ã€ç¼–ç æ•ˆç‡
"""

import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import json
import argparse
from tqdm import tqdm
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import transforms
from torch.utils.data import DataLoader
import lpips
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr
import sys
import os

# Add models to path (parent directory contains models/ and training/)
sys.path.append(str(Path(__file__).parent.parent / "models"))
sys.path.append(str(Path(__file__).parent.parent / "training"))

from taehv import TAEHV
from dataset import MiniDataset


class VAEEvaluator:
    """VAEæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path, config, device='cuda', num_samples=100):
        self.device = device
        self.num_samples = num_samples
        self.config = config
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¦ Loading model from: {model_path}")
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # æ‰“å°å¸§è£å‰ªä¿¡æ¯
        print(f"â„¹ï¸  TAEHV will trim {self.model.frames_to_trim} frames from decoded videos")
        print(f"   (e.g., {config.n_frames} input frames -> {config.n_frames - self.model.frames_to_trim} output frames)")
        
        # åˆå§‹åŒ–LPIPS (æ„ŸçŸ¥æŸå¤±)
        print("ğŸ”§ Initializing LPIPS metric...")
        self.lpips_fn = lpips.LPIPS(net='alex').to(device)
        
        # å­˜å‚¨ç»“æœ
        self.metrics = {
            'psnr': [],
            'ssim': [],
            'lpips': [],
            'mse': [],
            'mae': [],
        }
        
    def _load_model(self, model_path):
        """åŠ è½½VAEæ¨¡å‹"""
        # åˆå§‹åŒ–æ¨¡å‹æ¶æ„
        model = TAEHV(
            checkpoint_path=None,  # ä¸ä»é¢„è®­ç»ƒåŠ è½½ï¼Œåé¢ä¼šæ‰‹åŠ¨åŠ è½½æƒé‡
            patch_size=self.config.patch_size,
            latent_channels=self.config.latent_channels,
        ).to(self.device)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
            
        # ç§»é™¤module.å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
                
        model.load_state_dict(new_state_dict, strict=False)
        print(f"âœ… Model loaded successfully")
        
        return model
    
    def evaluate_batch(self, videos):
        """
        è¯„ä¼°ä¸€ä¸ªbatchçš„è§†é¢‘
        videos: [B, T, C, H, W] - å€¼èŒƒå›´åº”è¯¥æ˜¯ [0, 1]
        """
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥åœ¨ [0, 1] èŒƒå›´ï¼ˆTAEHV è¦æ±‚ï¼‰
            # å¦‚æœæ•°æ®åœ¨ [-1, 1] èŒƒå›´ï¼Œéœ€è¦è½¬æ¢
            if videos.min() < 0:
                videos = (videos + 1) / 2  # [-1, 1] -> [0, 1]
            
            # Encode - TAEHV ä½¿ç”¨ encode_video
            latents = self.model.encode_video(videos, parallel=True, show_progress_bar=False)
            
            # Decode - TAEHV ä½¿ç”¨ decode_video
            reconstructions = self.model.decode_video(latents, parallel=True, show_progress_bar=False)
            
            # é‡è¦ï¼šTAEHV decode_video ä¼šè£å‰ªå¸§æ•°ï¼
            # éœ€è¦å¯¹åŸå§‹è§†é¢‘è¿›è¡Œç›¸åŒçš„è£å‰ªä»¥å¯¹é½
            frames_to_trim = self.model.frames_to_trim
            if reconstructions.shape[1] < videos.shape[1]:
                # è£å‰ªåŸå§‹è§†é¢‘ï¼Œä½¿å…¶ä¸é‡å»ºè§†é¢‘çš„å¸§æ•°åŒ¹é…
                videos_trimmed = videos[:, frames_to_trim:frames_to_trim + reconstructions.shape[1]]
            else:
                videos_trimmed = videos
            
            # è®¡ç®—æŒ‡æ ‡
            batch_metrics = self._compute_metrics(videos_trimmed, reconstructions)
            
            return reconstructions, latents, batch_metrics
    
    def _compute_metrics(self, original, reconstructed):
        """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
        # è½¬æ¢åˆ°numpyç”¨äºæŸäº›æŒ‡æ ‡
        orig_np = original.cpu().numpy()
        recon_np = reconstructed.cpu().numpy()
        
        batch_size = original.shape[0]
        batch_metrics = {k: [] for k in self.metrics.keys()}
        
        for b in range(batch_size):
            # é€å¸§è®¡ç®—ï¼ˆå¯¹è§†é¢‘åºåˆ—ï¼‰
            for t in range(original.shape[1]):
                orig_frame = orig_np[b, t]
                recon_frame = recon_np[b, t]
                
                # 1. MSE (å‡æ–¹è¯¯å·®)
                mse = np.mean((orig_frame - recon_frame) ** 2)
                batch_metrics['mse'].append(mse)
                
                # 2. MAE (å¹³å‡ç»å¯¹è¯¯å·®)
                mae = np.mean(np.abs(orig_frame - recon_frame))
                batch_metrics['mae'].append(mae)
                
                # 3. PSNR (å³°å€¼ä¿¡å™ªæ¯”)
                # æ•°æ®èŒƒå›´å·²ç»æ˜¯ [0, 1]ï¼Œç›´æ¥ä½¿ç”¨
                # PSNR - è½¬ç½®ä¸º (H, W, C) æ ¼å¼
                psnr_val = psnr(
                    orig_frame.transpose(1, 2, 0),
                    recon_frame.transpose(1, 2, 0),
                    data_range=1.0
                )
                batch_metrics['psnr'].append(psnr_val)
                
                # 4. SSIM (ç»“æ„ç›¸ä¼¼æ€§)
                ssim_val = ssim(
                    orig_frame.transpose(1, 2, 0),
                    recon_frame.transpose(1, 2, 0),
                    data_range=1.0,
                    channel_axis=2,
                    win_size=11
                )
                batch_metrics['ssim'].append(ssim_val)
        
        # 5. LPIPS (æ„ŸçŸ¥ç›¸ä¼¼æ€§) - åœ¨æ•´ä¸ªbatchä¸Šè®¡ç®—
        with torch.no_grad():
            # Reshape: [B*T, C, H, W]
            B, T, C, H, W = original.shape
            orig_flat = original.reshape(B * T, C, H, W)
            recon_flat = reconstructed.reshape(B * T, C, H, W)
            
            lpips_val = self.lpips_fn(orig_flat, recon_flat)
            batch_metrics['lpips'].extend(lpips_val.cpu().numpy().flatten().tolist())
        
        return batch_metrics
    
    def evaluate_dataset(self, dataloader):
        """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†"""
        print(f"\nğŸ” Evaluating on {self.num_samples} samples...")
        
        sample_count = 0
        pbar = tqdm(total=self.num_samples, desc="Evaluating")
        
        for batch in dataloader:
            if sample_count >= self.num_samples:
                break
                
            # MiniDataset è¿”å›çš„æ˜¯ tensorï¼Œä¸æ˜¯å­—å…¸
            if isinstance(batch, dict):
                videos = batch['video'].to(self.device)
            else:
                videos = batch.to(self.device)
            
            # è¯„ä¼°
            reconstructions, latents, batch_metrics = self.evaluate_batch(videos)
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in self.metrics.keys():
                self.metrics[key].extend(batch_metrics[key])
            
            sample_count += videos.shape[0]
            pbar.update(videos.shape[0])
            
            # ä¿å­˜ä¸€äº›å¯è§†åŒ–æ ·æœ¬
            if sample_count <= 5:
                # å¯¹é½å¸§æ•°ï¼šè£å‰ªåŸå§‹è§†é¢‘ä»¥åŒ¹é…é‡å»ºè§†é¢‘
                frames_to_trim = self.model.frames_to_trim
                if reconstructions.shape[1] < videos.shape[1]:
                    videos_trimmed = videos[:, frames_to_trim:frames_to_trim + reconstructions.shape[1]]
                else:
                    videos_trimmed = videos
                
                self._save_visualization(
                    videos_trimmed, 
                    reconstructions, 
                    sample_idx=sample_count,
                    save_dir=Path("evaluation_results")
                )
        
        pbar.close()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        results = self._compute_statistics()
        
        return results
    
    def _compute_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        results = {}
        
        for metric_name, values in self.metrics.items():
            results[metric_name] = {
                'mean': float(np.mean(values)),
                'std': float(np.std(values)),
                'median': float(np.median(values)),
                'min': float(np.min(values)),
                'max': float(np.max(values)),
            }
        
        return results
    
    def _save_visualization(self, original, reconstructed, sample_idx, save_dir):
        """ä¿å­˜å¯è§†åŒ–ç»“æœ"""
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ä¸­é—´å¸§
        orig = original[0, original.shape[1]//2].cpu().numpy()
        recon = reconstructed[0, reconstructed.shape[1]//2].cpu().numpy()
        
        # æ•°æ®å·²ç»åœ¨ [0, 1] èŒƒå›´ï¼Œç›´æ¥è½¬æ¢åˆ° (H, W, C)
        orig = np.transpose(orig, (1, 2, 0))
        recon = np.transpose(recon, (1, 2, 0))
        
        # è®¡ç®—è¯¯å·®å›¾
        error = np.abs(orig - recon)
        
        # åˆ›å»ºå›¾åƒ
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        axes[0, 0].imshow(orig)
        axes[0, 0].set_title('Original')
        axes[0, 0].axis('off')
        
        axes[0, 1].imshow(recon)
        axes[0, 1].set_title('Reconstructed')
        axes[0, 1].axis('off')
        
        # è¯¯å·®çƒ­å›¾
        error_gray = np.mean(error, axis=2)
        im = axes[1, 0].imshow(error_gray, cmap='hot', vmin=0, vmax=0.3)
        axes[1, 0].set_title('Absolute Error Heatmap')
        axes[1, 0].axis('off')
        plt.colorbar(im, ax=axes[1, 0])
        
        # å¹¶æ’å¯¹æ¯”
        comparison = np.concatenate([orig, recon], axis=1)
        axes[1, 1].imshow(comparison)
        axes[1, 1].set_title('Original | Reconstructed')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_dir / f'sample_{sample_idx}.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def print_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š VAE Model Evaluation Results")
        print("="*60)
        
        # æŒ‡æ ‡è¯´æ˜
        metric_descriptions = {
            'psnr': 'PSNR (Peak Signal-to-Noise Ratio) - è¶Šé«˜è¶Šå¥½ (>30 excellent)',
            'ssim': 'SSIM (Structural Similarity) - è¶Šé«˜è¶Šå¥½ (>0.9 excellent)',
            'lpips': 'LPIPS (Perceptual Loss) - è¶Šä½è¶Šå¥½ (<0.1 excellent)',
            'mse': 'MSE (Mean Squared Error) - è¶Šä½è¶Šå¥½',
            'mae': 'MAE (Mean Absolute Error) - è¶Šä½è¶Šå¥½',
        }
        
        for metric_name, stats in results.items():
            print(f"\nğŸ“ˆ {metric_descriptions[metric_name]}")
            print(f"   Mean:   {stats['mean']:.4f}")
            print(f"   Std:    {stats['std']:.4f}")
            print(f"   Median: {stats['median']:.4f}")
            print(f"   Range:  [{stats['min']:.4f}, {stats['max']:.4f}]")
        
        # ç»¼åˆè¯„åˆ†
        print("\n" + "="*60)
        print("ğŸ¯ Overall Quality Assessment:")
        print("="*60)
        
        psnr_score = min(results['psnr']['mean'] / 35.0, 1.0)
        ssim_score = results['ssim']['mean']
        lpips_score = max(1.0 - results['lpips']['mean'] * 10, 0.0)
        
        overall_score = (psnr_score * 0.3 + ssim_score * 0.4 + lpips_score * 0.3) * 100
        
        print(f"   PSNR Score:    {psnr_score*100:.1f}/100")
        print(f"   SSIM Score:    {ssim_score*100:.1f}/100")
        print(f"   LPIPS Score:   {lpips_score*100:.1f}/100")
        print(f"   Overall Score: {overall_score:.1f}/100")
        
        if overall_score >= 85:
            quality = "ğŸŒŸ Excellent"
        elif overall_score >= 70:
            quality = "âœ… Good"
        elif overall_score >= 55:
            quality = "âš ï¸  Fair"
        else:
            quality = "âŒ Poor"
        
        print(f"\n   Quality Level: {quality}")
        print("="*60)
    
    def save_results(self, results, save_path):
        """ä¿å­˜ç»“æœåˆ°JSON"""
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ Results saved to: {save_path}")


def plot_metrics_distribution(evaluator, save_dir):
    """ç»˜åˆ¶æŒ‡æ ‡åˆ†å¸ƒå›¾"""
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
    
    for idx, (metric_name, values) in enumerate(evaluator.metrics.items()):
        if idx < len(axes):
            axes[idx].hist(values, bins=50, color=colors[idx], alpha=0.7, edgecolor='black')
            axes[idx].set_title(f'{metric_name.upper()} Distribution', fontsize=12, fontweight='bold')
            axes[idx].set_xlabel('Value')
            axes[idx].set_ylabel('Frequency')
            axes[idx].grid(True, alpha=0.3)
            
            # æ·»åŠ å‡å€¼çº¿
            mean_val = np.mean(values)
            axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.3f}')
            axes[idx].legend()
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(evaluator.metrics), len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'metrics_distribution.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š Metrics distribution plot saved to: {save_dir / 'metrics_distribution.png'}")


def compare_checkpoints(checkpoint_paths, config, dataloader, device='cuda'):
    """æ¯”è¾ƒå¤šä¸ªcheckpointsçš„æ€§èƒ½"""
    print("\nğŸ”„ Comparing multiple checkpoints...")
    
    results_comparison = {}
    
    for ckpt_path in checkpoint_paths:
        ckpt_name = Path(ckpt_path).name
        print(f"\nâ³ Evaluating: {ckpt_name}")
        
        evaluator = VAEEvaluator(ckpt_path, config, device=device, num_samples=50)
        results = evaluator.evaluate_dataset(dataloader)
        results_comparison[ckpt_name] = results
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    metrics_to_plot = ['psnr', 'ssim', 'lpips']
    
    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(15, 5))
    
    for idx, metric in enumerate(metrics_to_plot):
        checkpoint_names = list(results_comparison.keys())
        means = [results_comparison[name][metric]['mean'] for name in checkpoint_names]
        stds = [results_comparison[name][metric]['std'] for name in checkpoint_names]
        
        axes[idx].bar(range(len(checkpoint_names)), means, yerr=stds, capsize=5)
        axes[idx].set_xticks(range(len(checkpoint_names)))
        axes[idx].set_xticklabels(checkpoint_names, rotation=45, ha='right')
        axes[idx].set_title(f'{metric.upper()} Comparison')
        axes[idx].set_ylabel('Value')
        axes[idx].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('evaluation_results/checkpoint_comparison.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print("\nğŸ“Š Checkpoint comparison saved to: evaluation_results/checkpoint_comparison.png")
    
    return results_comparison


def main():
    parser = argparse.ArgumentParser(description='Evaluate VAE Model')
    parser.add_argument('--model_path', type=str, required=True, help='Path to model checkpoint')
    parser.add_argument('--config', type=str, default='training/configs/taehv_config_h100.py', help='Config file')
    parser.add_argument('--num_samples', type=int, default=100, help='Number of samples to evaluate')
    parser.add_argument('--batch_size', type=int, default=4, help='Batch size')
    parser.add_argument('--compare_checkpoints', action='store_true', help='Compare multiple checkpoints')
    parser.add_argument('--checkpoint_dir', type=str, help='Directory containing checkpoints to compare')
    parser.add_argument('--data_root', type=str, help='Override data root directory from config')
    parser.add_argument('--annotation_file', type=str, help='Override annotation file from config')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“‹ Loading config from: {args.config}")
    config_path = Path(args.config)
    
    # åŠ¨æ€å¯¼å…¥é…ç½®
    import importlib.util
    spec = importlib.util.spec_from_file_location("config", config_path)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    config = config_module.args
    
    # åˆ›å»ºæ•°æ®é›† (ä½¿ç”¨éªŒè¯é›†)
    print("\nğŸ“‚ Loading validation dataset...")
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    data_root = args.data_root if args.data_root else config.data_root
    annotation_file = args.annotation_file if args.annotation_file else config.annotation_file
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not Path(annotation_file).exists():
        print(f"âŒ Annotation file not found: {annotation_file}")
        print(f"\nğŸ’¡ Tip: å¦‚æœä½ æ²¡æœ‰åŸå§‹è®­ç»ƒæ•°æ®é›†ï¼Œä½ å¯ä»¥ï¼š")
        print(f"   1. ä½¿ç”¨ --annotation_file å’Œ --data_root å‚æ•°æŒ‡å®šå¯ç”¨çš„æ•°æ®é›†")
        print(f"   2. åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•æ•°æ®é›†ç”¨äºè¯„ä¼°")
        print(f"\n   ç¤ºä¾‹:")
        print(f"   python evaluate_vae.py \\")
        print(f"       --model_path ../output/xxx/final_model.pth \\")
        print(f"       --annotation_file /path/to/your/test_annotations.json \\")
        print(f"       --data_root /path/to/your/test_data")
        sys.exit(1)
    
    if not Path(data_root).exists():
        print(f"âŒ Data directory not found: {data_root}")
        print(f"\nğŸ’¡ Tip: ä½¿ç”¨ --data_root å‚æ•°æŒ‡å®šæ­£ç¡®çš„æ•°æ®ç›®å½•")
        sys.exit(1)
    
    try:
        dataset = MiniDataset(
            annotation_file=annotation_file,
            data_dir=data_root,
            patch_hw=max(config.height, config.width),  # ä½¿ç”¨è¾ƒå¤§çš„è¾¹ä½œä¸ºpatchå¤§å°
            n_frames=config.n_frames,
            augmentation=False  # è¯„ä¼°æ—¶ä¸åšæ•°æ®å¢å¼º
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        print(f"âœ… Loaded {len(dataset)} validation samples")
        
    except Exception as e:
        print(f"âŒ Failed to load dataset: {e}")
        print(f"\nğŸ’¡ è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®")
        sys.exit(1)
    
    # è¯„ä¼°æ¨¡å¼é€‰æ‹©
    if args.compare_checkpoints and args.checkpoint_dir:
        # æ¯”è¾ƒå¤šä¸ªcheckpoints
        checkpoint_dir = Path(args.checkpoint_dir)
        checkpoint_paths = sorted(checkpoint_dir.glob('checkpoint-*/pytorch_model.bin'))
        
        if not checkpoint_paths:
            checkpoint_paths = sorted(checkpoint_dir.glob('*.pth'))
        
        if not checkpoint_paths:
            print("âŒ No checkpoints found!")
            return
        
        print(f"Found {len(checkpoint_paths)} checkpoints to compare")
        results_comparison = compare_checkpoints(
            checkpoint_paths[:5],  # æœ€å¤šæ¯”è¾ƒ5ä¸ª
            config,
            dataloader,
            device='cuda'
        )
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        with open('evaluation_results/checkpoint_comparison.json', 'w') as f:
            json.dump(results_comparison, f, indent=2)
        
    else:
        # å•ä¸ªæ¨¡å‹è¯„ä¼°
        evaluator = VAEEvaluator(
            args.model_path,
            config,
            device='cuda',
            num_samples=args.num_samples
        )
        
        # è¿è¡Œè¯„ä¼°
        results = evaluator.evaluate_dataset(dataloader)
        
        # æ‰“å°ç»“æœ
        evaluator.print_results(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_results(results, 'evaluation_results/evaluation_results.json')
        
        # ç»˜åˆ¶åˆ†å¸ƒå›¾
        plot_metrics_distribution(evaluator, 'evaluation_results')


if __name__ == '__main__':
    main()

