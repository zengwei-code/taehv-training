#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„æ•°æ®èŒƒå›´å’Œæ¨¡å‹é…ç½®æ£€æŸ¥è„šæœ¬

ç”¨äºäº‹ååˆ†æcheckpointï¼Œè¯Šæ–­PSNRè¿‡ä½ç­‰é—®é¢˜ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/check_model_data_range.py \
        --model_path output/xxx/checkpoint-1000/model.pth \
        --config training/configs/taehv_config_a800.py \
        --data_root /path/to/data \
        --annotation_file /path/to/annotations.json \
        --output_dir ./check_results
"""

import argparse
import sys
import os
from pathlib import Path
import json
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
from torch.utils.data import DataLoader
import logging

from models.taehv import TAEHV
from training.dataset import MiniDataset


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    return logging.getLogger(__name__)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Check model data range and configuration")
    
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to trained model checkpoint (.pth file)"
    )
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to config file"
    )
    parser.add_argument(
        "--pretrained_path",
        type=str,
        default="checkpoints/taecvx.pth",
        help="Path to pretrained base model (default: checkpoints/taecvx.pth)"
    )
    parser.add_argument(
        "--data_root",
        type=str,
        help="Data directory (overrides config)"
    )
    parser.add_argument(
        "--annotation_file",
        type=str,
        help="Annotation file (overrides config)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./check_results",
        help="Output directory for check results"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=5,
        help="Number of samples to check"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=2,
        help="Batch size for checking"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0" if torch.cuda.is_available() else "cpu",
        help="Device to use"
    )
    
    return parser.parse_args()


def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(config_path)
    sys.path.append(str(config_path.parent))
    
    # åŠ¨æ€å¯¼å…¥é…ç½®
    import importlib.util
    spec = importlib.util.spec_from_file_location("config", config_path)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    
    return config_module.args


def check_model_config(config, logger) -> Dict[str, Any]:
    """æ£€æŸ¥æ¨¡å‹é…ç½®"""
    logger.info("="*79)
    logger.info("âš™ï¸  Model Configuration Check")
    logger.info("="*79)
    
    config_info = {}
    
    # æ•°æ®é…ç½®
    logger.info("\nğŸ“‚ Data Configuration:")
    data_config = {}
    for key in ['height', 'width', 'n_frames', 'batch_size', 'augmentation', 'data_root', 'annotation_file']:
        if hasattr(config, key):
            value = getattr(config, key)
            data_config[key] = value
            logger.info(f"  {key}: {value}")
    config_info['data'] = data_config
    
    # æ¨¡å‹é…ç½®
    logger.info("\nğŸ—ï¸  Model Configuration:")
    model_config = {}
    for key in ['model_type', 'use_seraena']:
        if hasattr(config, key):
            value = getattr(config, key)
            model_config[key] = value
            logger.info(f"  {key}: {value}")
    config_info['model'] = model_config
    
    # è®­ç»ƒé…ç½®
    logger.info("\nğŸ¯ Training Configuration:")
    train_config = {}
    for key in ['learning_rate', 'lr_scheduler', 'max_train_steps', 'gradient_accumulation_steps']:
        if hasattr(config, key):
            value = getattr(config, key)
            train_config[key] = value
            logger.info(f"  {key}: {value}")
    config_info['training'] = train_config
    
    logger.info("\n" + "="*79)
    
    return config_info


def check_data_range(input_videos: torch.Tensor, reconstructions: torch.Tensor, batch_idx: int) -> Dict[str, Any]:
    """æ£€æŸ¥æ•°æ®èŒƒå›´
    
    Args:
        input_videos: è¾“å…¥è§†é¢‘ tensor [B, T, C, H, W]
        reconstructions: é‡å»ºè§†é¢‘ tensor [B, T, C, H, W]
        batch_idx: æ‰¹æ¬¡ç´¢å¼•
        
    Returns:
        æ£€æŸ¥ç»“æœå­—å…¸
    """
    result = {
        "batch_idx": batch_idx,
        "input": {},
        "reconstruction": {},
        "errors": {},
        "warnings": []
    }
    
    # è¾“å…¥æ•°æ®ç»Ÿè®¡
    result["input"]["shape"] = list(input_videos.shape)
    result["input"]["dtype"] = str(input_videos.dtype)
    result["input"]["min"] = float(input_videos.min())
    result["input"]["max"] = float(input_videos.max())
    result["input"]["mean"] = float(input_videos.mean())
    result["input"]["std"] = float(input_videos.std())
    
    # é‡å»ºæ•°æ®ç»Ÿè®¡
    result["reconstruction"]["shape"] = list(reconstructions.shape)
    result["reconstruction"]["dtype"] = str(reconstructions.dtype)
    result["reconstruction"]["min"] = float(reconstructions.min())
    result["reconstruction"]["max"] = float(reconstructions.max())
    result["reconstruction"]["mean"] = float(reconstructions.mean())
    result["reconstruction"]["std"] = float(reconstructions.std())
    
    # è®¡ç®—è¯¯å·®ï¼ˆè°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…ï¼‰
    # å¦‚æœå¸§æ•°ä¸åŒï¼Œæˆªå–ç›¸åŒçš„å¸§æ•°
    min_frames = min(input_videos.shape[1], reconstructions.shape[1])
    input_aligned = input_videos[:, :min_frames]
    recon_aligned = reconstructions[:, :min_frames]
    
    mse = float(torch.nn.functional.mse_loss(input_aligned, recon_aligned))
    mae = float(torch.nn.functional.l1_loss(input_aligned, recon_aligned))
    max_diff = float((input_aligned - recon_aligned).abs().max())
    
    result["errors"]["mse"] = mse
    result["errors"]["mae"] = mae
    result["errors"]["rmse"] = mse ** 0.5
    result["errors"]["max_diff"] = max_diff
    
    # å¼‚å¸¸æ£€æµ‹
    warnings = []
    
    # æ£€æŸ¥1: è¾“å…¥èŒƒå›´
    if result["input"]["min"] < -0.1 or result["input"]["max"] > 1.1:
        warnings.append({
            "type": "input_range",
            "severity": "high",
            "message": f"Input range [{result['input']['min']:.4f}, {result['input']['max']:.4f}] outside expected [0, 1]"
        })
    
    # æ£€æŸ¥2: é‡å»ºèŒƒå›´
    if result["reconstruction"]["min"] < -0.1 or result["reconstruction"]["max"] > 1.1:
        warnings.append({
            "type": "recon_range",
            "severity": "high",
            "message": f"Reconstruction range [{result['reconstruction']['min']:.4f}, {result['reconstruction']['max']:.4f}] outside expected [0, 1]"
        })
    
    # æ£€æŸ¥3: é«˜MSE
    if mse > 0.1:
        warnings.append({
            "type": "high_mse",
            "severity": "medium",
            "message": f"High MSE: {mse:.6f} (explains low PSNR)"
        })
    
    # æ£€æŸ¥4: èŒƒå›´ä¸åŒ¹é…
    input_range = result["input"]["max"] - result["input"]["min"]
    recon_range = result["reconstruction"]["max"] - result["reconstruction"]["min"]
    if abs(input_range - recon_range) > 0.3:
        warnings.append({
            "type": "range_mismatch",
            "severity": "medium",
            "message": f"Range mismatch - Input: {input_range:.4f}, Recon: {recon_range:.4f}"
        })
    
    result["warnings"] = warnings
    
    return result


def print_check_result(result: Dict[str, Any], logger, detailed: bool = True):
    """æ‰“å°æ£€æŸ¥ç»“æœ"""
    logger.info("\n" + "="*79)
    logger.info(f"ğŸ” Data Range Check (Batch {result['batch_idx']})")
    logger.info("="*79)
    
    # è¾“å…¥æ•°æ®
    logger.info("\nğŸ“¥ Input Videos:")
    logger.info(f"  Shape: {result['input']['shape']}")
    logger.info(f"  Range: [{result['input']['min']:.4f}, {result['input']['max']:.4f}]  (Expected: [0, 1])")
    logger.info(f"  Mean: {result['input']['mean']:.4f} Â± {result['input']['std']:.4f}")
    logger.info(f"  Dtype: {result['input']['dtype']}")
    
    # é‡å»ºæ•°æ®
    logger.info("\nğŸ“¤ Reconstructions:")
    logger.info(f"  Shape: {result['reconstruction']['shape']}")
    logger.info(f"  Range: [{result['reconstruction']['min']:.4f}, {result['reconstruction']['max']:.4f}]")
    logger.info(f"  Mean: {result['reconstruction']['mean']:.4f} Â± {result['reconstruction']['std']:.4f}")
    logger.info(f"  Dtype: {result['reconstruction']['dtype']}")
    
    # è¯¯å·®
    logger.info("\nğŸ“Š Reconstruction Errors:")
    logger.info(f"  MSE:  {result['errors']['mse']:.6f}")
    logger.info(f"  RMSE: {result['errors']['rmse']:.6f}")
    logger.info(f"  MAE:  {result['errors']['mae']:.6f}")
    logger.info(f"  Max Diff: {result['errors']['max_diff']:.6f}")
    
    # ä¼°ç®—PSNR
    if result['errors']['mse'] > 0:
        psnr = -10 * torch.log10(torch.tensor(result['errors']['mse'])).item()
        logger.info(f"  Estimated PSNR: {psnr:.2f} dB")
    
    # è­¦å‘Š
    if result["warnings"]:
        logger.info(f"\nâš ï¸  Warnings ({len(result['warnings'])}):")
        for warn in result["warnings"]:
            severity_emoji = "ğŸ”´" if warn["severity"] == "high" else "ğŸŸ¡"
            logger.info(f"  {severity_emoji} [{warn['type']}] {warn['message']}")
    else:
        logger.info("\nâœ… No warnings - Data ranges look correct!")
    
    logger.info("="*79)


def main():
    logger = setup_logging()
    args = parse_args()
    
    logger.info("="*79)
    logger.info("ğŸ” Model Data Range and Configuration Checker")
    logger.info("="*79)
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"\nğŸ“‚ Loading configuration from {args.config}")
    config = load_config(args.config)
    
    # è¦†ç›–æ•°æ®è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.data_root:
        config.data_root = args.data_root
        logger.info(f"   Overriding data_root: {args.data_root}")
    if args.annotation_file:
        config.annotation_file = args.annotation_file
        logger.info(f"   Overriding annotation_file: {args.annotation_file}")
    
    # æ£€æŸ¥é…ç½®
    logger.info("\nâš™ï¸  Checking model configuration...")
    config_info = check_model_config(config, logger)
    
    # 2. åŠ è½½æ¨¡å‹
    logger.info(f"\nğŸ—ï¸  Loading model...")
    
    # é¦–å…ˆå°è¯•åŠ è½½é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹
    if Path(args.pretrained_path).exists():
        logger.info(f"   Loading pretrained base model from {args.pretrained_path}")
        try:
            model = TAEHV(checkpoint_path=args.pretrained_path)
            logger.info("   âœ… Pretrained base model loaded successfully")
        except Exception as e:
            logger.warning(f"   âš ï¸  Could not load pretrained model: {e}")
            logger.info("   Creating model without pretrained weights...")
            model = TAEHV(checkpoint_path=None)
    else:
        logger.warning(f"   âš ï¸  Pretrained model not found at {args.pretrained_path}")
        logger.info("   Creating model without pretrained weights...")
        model = TAEHV(checkpoint_path=None)
    
    # ç„¶ååŠ è½½è®­ç»ƒå¥½çš„ checkpointï¼ˆå¦‚æœæä¾›ï¼‰
    if args.model_path and Path(args.model_path).exists():
        logger.info(f"\nğŸ”„ Loading trained checkpoint from {args.model_path}")
        try:
            state_dict = torch.load(args.model_path, map_location='cpu')
            model.load_state_dict(state_dict)
            logger.info("   âœ… Trained checkpoint loaded successfully")
        except Exception as e:
            logger.error(f"   âŒ Error loading trained checkpoint: {e}")
            logger.info("   Continuing with pretrained weights only...")
    elif args.model_path:
        logger.error(f"   âŒ Trained checkpoint not found: {args.model_path}")
        logger.info("   Continuing with pretrained weights only...")
    
    model = model.to(args.device)
    model.eval()
    
    # 3. åŠ è½½æ•°æ®é›†
    logger.info(f"\nğŸ“Š Loading dataset...")
    logger.info(f"   Data root: {config.data_root}")
    logger.info(f"   Annotation file: {config.annotation_file}")
    
    try:
        dataset = MiniDataset(
            annotation_file=config.annotation_file,
            data_dir=config.data_root,
            patch_hw=getattr(config, 'height', 480),
            n_frames=getattr(config, 'n_frames', 12),
            augmentation=False
        )
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if len(dataset.annotations) > args.num_samples:
            dataset.annotations = dataset.annotations[:args.num_samples]
        
        dataloader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=2
        )
        
        logger.info(f"   âœ… Loaded {len(dataset)} samples")
        
    except Exception as e:
        logger.error(f"   âŒ Failed to load dataset: {e}")
        logger.error(f"   Please check data_root and annotation_file paths")
        import traceback
        traceback.print_exc()
        return
    
    # 4. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 5. æ‰§è¡Œæ£€æŸ¥
    logger.info(f"\nğŸ” Performing data range checks on {args.num_samples} samples...")
    logger.info("="*79)
    
    all_results = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            logger.info(f"\nProcessing batch {batch_idx + 1}/{len(dataloader)}...")
            
            # é¢„å¤„ç† - MiniDatasetè¾“å‡ºuint8 [0,255]ï¼Œéœ€è¦å½’ä¸€åŒ–
            videos = batch.float() / 255.0  # å½’ä¸€åŒ–åˆ° [0,1]
            videos = videos.to(args.device)
            
            # æ¨¡å‹æ¨ç†
            try:
                encoded = model.encode_video(videos, parallel=True, show_progress_bar=False)
                decoded = model.decode_video(encoded, parallel=True, show_progress_bar=False)
                
                # æ‰§è¡Œæ£€æŸ¥
                result = check_data_range(videos, decoded, batch_idx)
                
                # æ‰“å°ç»“æœï¼ˆç¬¬ä¸€ä¸ªbatchæ‰“å°è¯¦ç»†ä¿¡æ¯ï¼‰
                print_check_result(result, logger, detailed=(batch_idx == 0))
                
                # ä¿å­˜ç»“æœ
                result_path = output_dir / f"batch_{batch_idx}_result.json"
                with open(result_path, 'w') as f:
                    json.dump(result, f, indent=2)
                
                all_results.append(result)
                
            except Exception as e:
                logger.error(f"   âŒ Error processing batch {batch_idx}: {e}")
                import traceback
                traceback.print_exc()
    
    # 6. æ±‡æ€»ç»“æœ
    logger.info("\n" + "="*79)
    logger.info("ğŸ“Š Summary of All Checks")
    logger.info("="*79)
    
    if all_results:
        # ç»Ÿè®¡è­¦å‘Š
        total_warnings = sum(len(r["warnings"]) for r in all_results)
        high_severity_warnings = sum(
            len([w for w in r["warnings"] if w["severity"] == "high"])
            for r in all_results
        )
        
        logger.info(f"\nTotal batches checked: {len(all_results)}")
        logger.info(f"Total warnings: {total_warnings}")
        logger.info(f"High severity warnings: {high_severity_warnings}")
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡
        avg_input_range = sum(r["input"]["max"] - r["input"]["min"] for r in all_results) / len(all_results)
        avg_recon_range = sum(r["reconstruction"]["max"] - r["reconstruction"]["min"] for r in all_results) / len(all_results)
        
        logger.info(f"\nAverage input range: {avg_input_range:.4f}")
        logger.info(f"Average reconstruction range: {avg_recon_range:.4f}")
        
        # è¯¯å·®ç»Ÿè®¡
        avg_mse = sum(r["errors"]["mse"] for r in all_results) / len(all_results)
        avg_mae = sum(r["errors"]["mae"] for r in all_results) / len(all_results)
        
        logger.info(f"\nAverage MSE: {avg_mse:.6f}")
        logger.info(f"Average MAE: {avg_mae:.6f}")
        
        # ä¼°ç®—PSNR
        if avg_mse > 0:
            estimated_psnr = -10 * torch.log10(torch.tensor(avg_mse)).item()
            logger.info(f"Estimated PSNR: {estimated_psnr:.2f} dB")
        
        # æ‰“å°æ‰€æœ‰é«˜ä¸¥é‡æ€§è­¦å‘Š
        if high_severity_warnings > 0:
            logger.info(f"\nâš ï¸  High Severity Warnings:")
            for idx, result in enumerate(all_results):
                high_warns = [w for w in result["warnings"] if w["severity"] == "high"]
                for warn in high_warns:
                    logger.info(f"   Batch {idx}: [{warn['type']}] {warn['message']}")
        
        # ä¿å­˜æ±‡æ€»
        summary = {
            "config": config_info,
            "num_batches": len(all_results),
            "total_warnings": total_warnings,
            "high_severity_warnings": high_severity_warnings,
            "avg_input_range": float(avg_input_range),
            "avg_recon_range": float(avg_recon_range),
            "avg_mse": float(avg_mse),
            "avg_mae": float(avg_mae),
            "estimated_psnr": float(estimated_psnr) if avg_mse > 0 else None
        }
        
        summary_path = output_dir / "summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        logger.info(f"\nğŸ’¾ Saved summary to {summary_path}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_path = output_dir / "full_check_results.json"
        with open(full_results_path, 'w') as f:
            json.dump({
                "summary": summary,
                "batch_results": all_results
            }, f, indent=2)
        logger.info(f"ğŸ’¾ Saved full results to {full_results_path}")
    
    logger.info("\n" + "="*79)
    logger.info("âœ… Check completed!")
    logger.info(f"ğŸ“ Results saved to: {output_dir}")
    logger.info("="*79)


if __name__ == "__main__":
    main()
