#!/usr/bin/env python3
"""
TAEHVè®­ç»ƒè„šæœ¬
åŸºäºaccelerate + deepspeedçš„åˆ†å¸ƒå¼è®­ç»ƒ
"""

import argparse
import datetime
import json
import logging
import math
import os
import shutil
import sys
import warnings
from pathlib import Path
from typing import Optional

import numpy as np

# è¿‡æ»¤ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", message=".*expandable_segments not supported.*")
warnings.filterwarnings("ignore", message=".*kernel version.*below.*recommended.*")
warnings.filterwarnings("ignore", category=FutureWarning, module="torch.cuda.amp")
warnings.filterwarnings("ignore", message=".*NCCL_BLOCKING_WAIT is deprecated.*")
warnings.filterwarnings("ignore", message=".*Setting OMP_NUM_THREADS.*")
warnings.filterwarnings("ignore", message=".*pymp-.*", category=UserWarning)

# è¿‡æ»¤CUDAåˆ†é…å™¨è­¦å‘Š
warnings.filterwarnings("ignore", message=".*expandable_segments not supported on this platform.*")
warnings.filterwarnings("ignore", message=".*CUDAAllocatorConfig.*expandable_segments.*")

# è®¾ç½®ç¯å¢ƒå˜é‡å‡å°‘CUDAè­¦å‘Š
os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

# æ·»åŠ é¢œè‰²æ”¯æŒ
class ColoredFormatter(logging.Formatter):
    """å¸¦é¢œè‰²çš„æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    # é¢œè‰²å®šä¹‰
    COLORS = {
        'DEBUG': '\033[36m',      # é’è‰²
        'INFO': '\033[32m',       # ç»¿è‰²
        'WARNING': '\033[33m',    # é»„è‰²
        'ERROR': '\033[31m',      # çº¢è‰²
        'CRITICAL': '\033[35m',   # ç´«è‰²
    }
    RESET = '\033[0m'
    
    def format(self, record):
        # è·å–åŸºç¡€æ ¼å¼åŒ–å†…å®¹
        log_message = super().format(record)
        
        # æ·»åŠ é¢œè‰²
        if record.levelname in self.COLORS:
            log_message = f"{self.COLORS[record.levelname]}{log_message}{self.RESET}"
        
        return log_message

import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import DataLoader

import accelerate
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration, set_seed, DistributedType

import diffusers
from diffusers.optimization import get_scheduler
from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available

from tqdm.auto import tqdm
import transformers

# éªŒè¯ç›¸å…³åº“
import lpips
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

# æ£€æŸ¥ç‰ˆæœ¬
check_min_version("0.21.0")

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.taehv import TAEHV
from training.dataset import MiniDataset
from training.training_utils import get_ref_vae

# é…ç½®å½©è‰²æ—¥å¿—
logger = get_logger(__name__, log_level="INFO")

# è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘DEBUGä¿¡æ¯
logging.getLogger("accelerate.tracking").setLevel(logging.WARNING)  # å‡å°‘TensorBoard DEBUGä¿¡æ¯
logging.getLogger("transformers").setLevel(logging.WARNING)  # å‡å°‘transformersçš„WARNING

# ä¸ºä¸»loggeræ·»åŠ é¢œè‰²æ”¯æŒ
console_handler = logging.StreamHandler()
console_handler.setFormatter(ColoredFormatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
))
logging.getLogger().handlers.clear()  # æ¸…é™¤é»˜è®¤å¤„ç†å™¨
logging.getLogger().addHandler(console_handler)
logging.getLogger().setLevel(logging.INFO)


def parse_args():
    parser = argparse.ArgumentParser(description="Simple example of a training script.")
    parser.add_argument(
        "--config", type=str, required=True, help="Path to the config file"
    )
    parser.add_argument(
        "--logging_dir",
        type=str,
        default="logs",
        help="Logging directory",
    )
    parser.add_argument(
        "--mixed_precision",
        type=str,
        default=None,
        choices=["no", "fp16", "bf16"],
        help="Whether to use mixed precision.",
    )
    parser.add_argument(
        "--report_to",
        type=str,
        default="tensorboard",
        help="Report to which service.",
    )
    parser.add_argument(
        "--local_rank", type=int, default=-1, help="For distributed training: local_rank"
    )

    args = parser.parse_args()
    return args


def compute_validation_metrics(
    model,
    val_dataloader,
    device,
    lpips_fn,
    num_samples=20,
    logger=None,
    use_amp=True
):
    """
    è®¡ç®—éªŒè¯é›†ä¸Šçš„è¯„ä¼°æŒ‡æ ‡
    
    Args:
        model: TAEHVæ¨¡å‹
        val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        lpips_fn: LPIPSæŸå¤±å‡½æ•°
        num_samples: è¯„ä¼°æ ·æœ¬æ•°
        logger: æ—¥å¿—è®°å½•å™¨
        use_amp: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆä¿®å¤bf16ç±»å‹ä¸åŒ¹é…é—®é¢˜ï¼‰
    
    Returns:
        dict: {
            'val/psnr': float,
            'val/ssim': float,
            'val/lpips': float,
            'val/psnr_std': float,
            'val/ssim_std': float,
        }
    """
    if logger:
        logger.info(f"Computing validation metrics on {num_samples} samples...")
    
    model.eval()
    
    psnr_list = []
    ssim_list = []
    lpips_list = []
    
    with torch.no_grad():
        sample_count = 0
        for batch in val_dataloader:
            if sample_count >= num_samples:
                break
            
            # è·å–è§†é¢‘æ•°æ®
            if isinstance(batch, dict):
                videos = batch['video'].to(device)
            else:
                videos = batch.to(device)
            
            # ç¡®ä¿è¾“å…¥åœ¨ [0, 1] èŒƒå›´ï¼ˆTAEHV è¦æ±‚ï¼‰
            if videos.min() < 0:
                videos = (videos + 1) / 2  # [-1, 1] -> [0, 1]
            
            try:
                # âœ… ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦åŒ…è£¹ï¼Œè§£å†³bf16ç±»å‹ä¸åŒ¹é…é—®é¢˜
                with torch.amp.autocast('cuda', enabled=use_amp, dtype=torch.bfloat16):
                    # ç¼–ç -è§£ç 
                    latents = model.encode_video(videos, parallel=True, show_progress_bar=False)
                    reconstructions = model.decode_video(latents, parallel=True, show_progress_bar=False)
                
                # âœ… åœ¨autocaståï¼Œè½¬æ¢å›float32ï¼ˆscikit-imageä¸æ”¯æŒbfloat16ï¼‰
                reconstructions = reconstructions.float()
                
                # å¯¹é½å¸§æ•°ï¼ˆTAEHVä¼šè£å‰ªå¸§ï¼‰
                frames_to_trim = getattr(model, 'frames_to_trim', 0)
                if reconstructions.shape[1] < videos.shape[1] and frames_to_trim > 0:
                    # è£å‰ªåŸå§‹è§†é¢‘ï¼Œä½¿å…¶ä¸é‡å»ºè§†é¢‘çš„å¸§æ•°åŒ¹é…
                    videos_trimmed = videos[:, frames_to_trim:frames_to_trim + reconstructions.shape[1]]
                else:
                    videos_trimmed = videos
                
                # ç¡®ä¿å¸§æ•°åŒ¹é…
                if videos_trimmed.shape[1] != reconstructions.shape[1]:
                    if logger:
                        logger.warning(f"Frame mismatch: original {videos_trimmed.shape[1]} vs recon {reconstructions.shape[1]}, skipping batch")
                    continue
                
                # è½¬æ¢ä¸ºnumpyè®¡ç®—PSNR/SSIMï¼ˆç¡®ä¿float32ï¼‰
                orig_np = videos_trimmed.cpu().float().numpy()
                recon_np = reconstructions.cpu().numpy()
                
                # é€å¸§è®¡ç®—PSNRå’ŒSSIM
                batch_size, n_frames = orig_np.shape[0], orig_np.shape[1]
                for b in range(batch_size):
                    for t in range(n_frames):
                        orig_frame = orig_np[b, t].transpose(1, 2, 0)  # CHW -> HWC
                        recon_frame = recon_np[b, t].transpose(1, 2, 0)
                        
                        # PSNR
                        psnr_val = psnr(orig_frame, recon_frame, data_range=1.0)
                        psnr_list.append(psnr_val)
                        
                        # SSIM
                        ssim_val = ssim(
                            orig_frame, 
                            recon_frame, 
                            data_range=1.0, 
                            channel_axis=2, 
                            win_size=11
                        )
                        ssim_list.append(ssim_val)
                
                # æ‰¹é‡è®¡ç®—LPIPSï¼ˆåœ¨GPUä¸Šæ›´å¿«ï¼‰
                if lpips_fn is not None:
                    B, T, C, H, W = videos_trimmed.shape
                    # ç¡®ä¿ä¸¤ä¸ªè¾“å…¥éƒ½æ˜¯float32ï¼ˆLPIPSä¸æ”¯æŒbf16ï¼‰
                    orig_flat = videos_trimmed.float().reshape(B * T, C, H, W)
                    recon_flat = reconstructions.float().reshape(B * T, C, H, W)
                    
                    # LPIPSå¿…é¡»åœ¨float32ä¸‹è®¡ç®—ï¼Œä¸ä½¿ç”¨autocast
                    lpips_vals = lpips_fn(orig_flat, recon_flat)
                    lpips_list.extend(lpips_vals.cpu().numpy().flatten().tolist())
                
                sample_count += batch_size
                
            except Exception as e:
                if logger:
                    logger.warning(f"Error processing batch: {e}, skipping")
                continue
    
    model.train()
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    results = {
        'val/psnr': float(np.mean(psnr_list)),
        'val/ssim': float(np.mean(ssim_list)),
        'val/psnr_std': float(np.std(psnr_list)),
        'val/ssim_std': float(np.std(ssim_list)),
    }
    
    if lpips_list:
        results['val/lpips'] = float(np.mean(lpips_list))
    
    if logger:
        logger.info(f"Validation completed: PSNR={results['val/psnr']:.2f}, SSIM={results['val/ssim']:.4f}")
    
    return results


def main():
    args = parse_args()

    # å¯¼å…¥é…ç½®æ–‡ä»¶
    config_path = Path(args.config)
    sys.path.append(str(config_path.parent))
    config_module = config_path.stem
    config = __import__(config_module).args

    # åˆå§‹åŒ–accelerator
    accelerator_project_config = ProjectConfiguration(
        project_dir=config.output_dir, logging_dir=args.logging_dir
    )

    accelerator = Accelerator(
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision or config.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
    )

    # è®¾ç½®logging - å¯ç”¨DEBUGä»¥æŸ¥çœ‹Seraenaç»´åº¦è°ƒè¯•ä¿¡æ¯
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.DEBUG,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # è®¾ç½®éšæœºç§å­
    if config.seed is not None:
        set_seed(config.seed)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_main_process:
        if config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    logger.info("Loading TAEHV model...")
    model = TAEHV(
        checkpoint_path=config.pretrained_model_path,
        patch_size=config.patch_size,
        latent_channels=config.latent_channels
    )

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if config.gradient_checkpointing:
        model.encoder.requires_grad_(True)
        model.decoder.requires_grad_(True)

    # å‡†å¤‡ä¼˜åŒ–å™¨
    logger.info("Setting up optimizer...")
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        betas=(config.beta1, config.beta2),
        weight_decay=config.weight_decay,
        eps=config.epsilon,
    )

    # å‡†å¤‡æ•°æ®é›†
    logger.info("Loading dataset...")
    train_dataset = MiniDataset(
        annotation_file=config.annotation_file,
        data_dir=config.data_root,
        patch_hw=config.height,
        n_frames=config.n_frames,
        min_frame_delta=config.min_frame_delta,
        max_frame_delta=config.max_frame_delta,
        augmentation=True
    )

    # æ„å»ºDataLoaderå‚æ•°ï¼ˆå¤„ç†å•è¿›ç¨‹/å¤šè¿›ç¨‹æ¨¡å¼çš„å…¼å®¹æ€§ï¼‰
    train_dataloader_kwargs = {
        'shuffle': True,
        'batch_size': config.train_batch_size,
        'num_workers': config.dataloader_num_workers,
        'pin_memory': config.pin_memory,
        'drop_last': True,
    }
    
    # prefetch_factor ä»…åœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹æœ‰æ•ˆ
    if config.dataloader_num_workers > 0 and hasattr(config, 'prefetch_factor'):
        train_dataloader_kwargs['prefetch_factor'] = config.prefetch_factor
    
    train_dataloader = DataLoader(train_dataset, **train_dataloader_kwargs)

    # åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨ç›¸åŒé…ç½®ä½†ä¸åšå¢å¼ºï¼‰
    logger.info("Creating validation dataset...")
    val_dataset = MiniDataset(
        annotation_file=config.annotation_file,
        data_dir=config.data_root,
        patch_hw=config.height,
        n_frames=config.n_frames,
        min_frame_delta=config.min_frame_delta,
        max_frame_delta=config.max_frame_delta,
        augmentation=False,  # éªŒè¯æ—¶ä¸åšæ•°æ®å¢å¼º
        cache_videos=False
    )
    
    # é™åˆ¶éªŒè¯é›†å¤§å°ï¼ˆé¿å…è¿‡æ…¢ï¼‰
    if len(val_dataset.annotations) > 100:
        val_dataset.annotations = val_dataset.annotations[:100]
        logger.info(f"Limited validation dataset to 100 samples")
    else:
        logger.info(f"Validation dataset has {len(val_dataset.annotations)} samples")
    
    # éªŒè¯DataLoaderä¸è®­ç»ƒä¿æŒä¸€è‡´çš„æ¨¡å¼ï¼ˆå•è¿›ç¨‹/å¤šè¿›ç¨‹ï¼‰
    val_num_workers = 0 if config.dataloader_num_workers == 0 else min(2, config.dataloader_num_workers)
    val_dataloader = DataLoader(
        val_dataset,
        shuffle=False,  # éªŒè¯ä¸éœ€è¦shuffle
        batch_size=config.validation_batch_size,
        num_workers=val_num_workers,  # ä¸è®­ç»ƒé…ç½®ä¸€è‡´
        pin_memory=True,
        drop_last=False  # éªŒè¯ä¸éœ€è¦drop_last
    )
    logger.info(f"âœ… Validation dataloader created (num_workers={val_num_workers})")

    # å‡†å¤‡å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        config.lr_scheduler,
        optimizer=optimizer,
        # num_warmup_steps=config.lr_warmup_steps * accelerator.num_processes,
        # num_training_steps=config.max_train_steps * accelerator.num_processes,
        num_warmup_steps=config.lr_warmup_steps,
        num_training_steps=config.max_train_steps,
        num_cycles=config.lr_num_cycles,
    )

    # å‡†å¤‡å‚è€ƒVAEï¼ˆå¦‚æœéœ€è¦ï¼‰
    vae_ref = None
    if config.use_ref_vae:
        logger.info("Loading reference VAE...")
        vae_ref = get_ref_vae(
            device="cpu", 
            dtype=getattr(torch, config.ref_vae_dtype),
            cogvideox_model_path=config.cogvideox_model_path
        )
        vae_ref.requires_grad_(False)

    # å‡†å¤‡Seraenaï¼ˆå¦‚æœéœ€è¦ï¼‰
    seraena = None
    if config.use_seraena:
        logger.info("Loading Seraena adversarial trainer...")
        from models.seraena import Seraena
        # ä½¿ç”¨4é€šé“åŒ¹é…TAEHVå®é™…çš„latenté€šé“æ•°
        seraena = Seraena(3 * config.n_seraena_frames, 4)

    # å‡†å¤‡accelerator
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    if vae_ref is not None:
        # å‚è€ƒVAEä¸éœ€è¦è®­ç»ƒï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡è€Œä¸æ˜¯é€šè¿‡DeepSpeedå‡†å¤‡
        vae_ref = vae_ref.to(accelerator.device)
    
    # æ³¨å†Œcheckpointä¿å­˜hooks - ç¡®ä¿æ¨¡å‹çŠ¶æ€æ­£ç¡®ä¿å­˜
    def save_model_hook(models, weights, output_dir):
        if accelerator.is_main_process:
            # ä¿å­˜ä¸»æ¨¡å‹çŠ¶æ€
            model_to_save = accelerator.unwrap_model(model)
            torch.save(model_to_save.state_dict(), os.path.join(output_dir, "model.pth"))
            logger.info(f"Saved model state dict to {output_dir}/model.pth")

    def load_model_hook(models, input_dir):
        # æ¨¡å‹åŠ è½½é€»è¾‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        pass
    
    # æ³¨å†Œhooks
    accelerator.register_save_state_pre_hook(save_model_hook)
    accelerator.register_load_state_pre_hook(load_model_hook)
    
    if seraena is not None:
        # Seraenaä¸éœ€è¦è®­ç»ƒï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡è€Œä¸æ˜¯é€šè¿‡DeepSpeedå‡†å¤‡
        seraena = seraena.to(accelerator.device)

    # åˆå§‹åŒ–LPIPSæ„ŸçŸ¥æŸå¤±ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    lpips_fn = None
    if accelerator.is_main_process:
        logger.info("Initializing LPIPS metric for validation...")
        lpips_fn = lpips.LPIPS(net='alex').to(accelerator.device)
        lpips_fn.eval()  # è®¾ç½®ä¸ºevalæ¨¡å¼
        logger.info("âœ… LPIPS initialized")

    # åˆå§‹åŒ–æœ€ä½³æ¨¡å‹è·Ÿè¸ªå˜é‡
    best_val_psnr = 0.0
    best_val_step = 0
    patience_counter = 0
    patience_limit = getattr(config, 'early_stopping_patience', 5)  # é»˜è®¤5æ¬¡éªŒè¯æ— æ”¹å–„åˆ™åœæ­¢

    # éªŒè¯å†å²è®°å½•
    validation_history = {
        'steps': [],
        'psnr': [],
        'ssim': [],
        'lpips': []
    }
    logger.info(f"Early stopping patience: {patience_limit} validations")

    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / config.gradient_accumulation_steps
    )
    if config.max_train_steps is None:
        config.max_train_steps = config.num_train_epochs * num_update_steps_per_epoch

    # æˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®—æˆ‘ä»¬çš„è®­ç»ƒè½®æ•°ä½œä¸ºæˆ‘ä»¬çš„è®­ç»ƒæ­¥æ•°çš„å‡½æ•°
    config.num_train_epochs = math.ceil(config.max_train_steps / num_update_steps_per_epoch)

    # åˆå§‹åŒ–tracker
    if accelerator.is_main_process:
        # è¿‡æ»¤é…ç½®ï¼Œåªä¿ç•™TensorBoardæ”¯æŒçš„ç±»å‹ (int, float, str, bool, torch.Tensor)
        filtered_config = {}
        for key, value in vars(config).items():
            if isinstance(value, (int, float, str, bool)) or (hasattr(value, 'dtype') and hasattr(value, 'device')):
                filtered_config[key] = value
            elif isinstance(value, dict):
                # å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered_config[key] = str(value)
            else:
                # å°†å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered_config[key] = str(value)
        
        accelerator.init_trackers(config.tracker_name, config=filtered_config)

    # è®­ç»ƒå¾ªç¯
    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num Epochs = {config.num_train_epochs}")
    logger.info(f"  Instantaneous batch size per device = {config.train_batch_size}")
    logger.info(
        f"  Total train batch size (w. parallel, distributed & accumulation) = "
        f"{config.train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps}"
    )
    logger.info(f"  Gradient Accumulation steps = {config.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {config.max_train_steps}")

    global_step = 0
    first_epoch = 0

    # å¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤
    if config.resume_from_checkpoint:
        if config.resume_from_checkpoint != "latest":
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´è·¯å¾„ï¼ˆåŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼‰
            if os.path.sep in config.resume_from_checkpoint or '/' in config.resume_from_checkpoint:
                # å®Œæ•´è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                checkpoint_path = config.resume_from_checkpoint
                path = os.path.basename(config.resume_from_checkpoint)  # ç”¨äºæå–æ­¥æ•°
            else:
                # åªæ˜¯checkpointåç§°ï¼Œéœ€è¦ç»„åˆè·¯å¾„
                path = config.resume_from_checkpoint
                checkpoint_path = os.path.join(config.output_dir, path)
        else:
            # è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹
            dirs = os.listdir(config.output_dir)
            dirs = [d for d in dirs if d.startswith("checkpoint")]
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
            path = dirs[-1] if len(dirs) > 0 else None
            checkpoint_path = os.path.join(config.output_dir, path) if path else None

        # æ£€æŸ¥checkpointæ˜¯å¦å­˜åœ¨
        if path is None or not os.path.exists(checkpoint_path):
            accelerator.print(
                f"Checkpoint '{config.resume_from_checkpoint}' does not exist. Starting a new training run."
            )
            config.resume_from_checkpoint = None
            initial_global_step = 0
        else:
            accelerator.print(f"Resuming from checkpoint {path}")
            accelerator.load_state(checkpoint_path)
            global_step = int(path.split("-")[1])

            initial_global_step = global_step
            first_epoch = global_step // num_update_steps_per_epoch
    else:
        initial_global_step = 0

    progress_bar = tqdm(
        range(0, config.max_train_steps),
        initial=initial_global_step,
        desc="Steps",
        disable=not accelerator.is_local_main_process,
    )

    # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹çš„æ¨¡å‹éƒ½å¤„äºè®­ç»ƒæ¨¡å¼
    model.train()
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    accelerator.wait_for_everyone()
    
    # æç¤ºGPUå†…å­˜ç›‘æ§å·²å¯ç”¨
    if accelerator.is_main_process:
        logger.info("GPU memory monitoring enabled (every 50 steps)")

    # è®­ç»ƒä¸»å¾ªç¯
    for epoch in range(first_epoch, config.num_train_epochs):
        model.train()
        train_loss = 0.0
        
        for step, batch in enumerate(train_dataloader):
            # æ‰“å°ç¬¬ä¸€ä¸ªbatchçš„ä¿¡æ¯ï¼ˆä»…ä¸€æ¬¡ï¼‰
            if global_step == 0 and step == 0 and accelerator.is_main_process:
                logger.info(f"First batch shape: {batch.shape}")
            
            with accelerator.accumulate(model):
                # æ•°æ®é¢„å¤„ç† - MiniDatasetå·²å½’ä¸€åŒ–åˆ°[0,1]
                frames = batch.float()  # N,T,C,H,W, [0,1] - no need to divide by 255
                # è½¬æ¢åˆ°ä¸æ¨¡å‹å‚æ•°ç›¸åŒçš„ç±»å‹ï¼ˆbf16ï¼‰
                frames = frames.to(accelerator.device, dtype=torch.bfloat16)
                
                # å‰å‘ä¼ æ’­
                with accelerator.autocast():
                    # ç¼–ç 
                    encoded = model.encode_video(frames, parallel=True, show_progress_bar=False)
                    
                    # è§£ç 
                    # æ£€æŸ¥CogVideoXæ¨¡å¼æ˜¯å¦ä¼šè·³è¿‡è£å‰ª
                    # é€»è¾‘ä¸models/taehv.py:228ä¿æŒä¸€è‡´
                    will_skip_trim = model.is_cogvideox and encoded.shape[1] % 2 == 0
                    if will_skip_trim:
                        # CogVideoXæ¨¡å¼ä¸”ç¼–ç å¸§æ•°ä¸ºå¶æ•°ï¼Œè§£ç æ—¶ä¸è£å‰ªï¼Œtargetä¹Ÿä¸è£å‰ª
                        frames_target = frames
                    else:
                        # æ­£å¸¸æ¨¡å¼ï¼Œtargetéœ€è¦è£å‰ª
                        frames_target = frames[:, :-model.frames_to_trim] if model.frames_to_trim > 0 else frames
                    
                    decoded = model.decode_video(encoded, parallel=True, show_progress_bar=False)
                    
                    # é‡å»ºæŸå¤±
                    recon_loss = F.mse_loss(decoded, frames_target)
                    
                    # ç¼–ç æŸå¤±ï¼ˆå¦‚æœæœ‰å‚è€ƒVAEï¼‰
                    total_loss = recon_loss
                    if vae_ref is not None:
                        try:
                            with torch.no_grad():
                                ref_latent = vae_ref.encode_video(frames)
                            encoder_loss = F.mse_loss(encoded, ref_latent)
                            total_loss = (
                                config.reconstruction_loss_weight * recon_loss + 
                                config.encoder_loss_weight * encoder_loss
                            )
                        except Exception as e:
                            logger.warning(f"VAE encoding failed: {e}, using only reconstruction loss")
                            total_loss = recon_loss
                    
                    # Seraenaå¯¹æŠ—æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if seraena is not None:
                        try:
                            # é‡ç»„ä¸ºseraenaè¾“å…¥æ ¼å¼
                            def pad_and_group(x):
                                # x: N, T, C, H, W -> N*T//n_seraena_frames, n_seraena_frames*C, H, W
                                logger.debug(f"pad_and_group input shape: {x.shape}")
                                
                                frames_to_trim = getattr(model, 'frames_to_trim', 0)
                                logger.debug(f"frames_to_trim: {frames_to_trim}")
                                
                                if frames_to_trim > 0:
                                    x_padded = torch.cat([x, x[:, :frames_to_trim]], 1)
                                else:
                                    x_padded = x
                                    
                                n, t, c, h, w = x_padded.shape
                                logger.debug(f"After padding: {x_padded.shape}")
                                logger.debug(f"n_seraena_frames: {config.n_seraena_frames}")
                                
                                # ç¡®ä¿å¸§æ•°èƒ½è¢«n_seraena_framesæ•´é™¤
                                if t % config.n_seraena_frames != 0:
                                    logger.warning(f"Frame count {t} is not divisible by n_seraena_frames {config.n_seraena_frames}")
                                    # è£å‰ªåˆ°æœ€æ¥è¿‘çš„å¯æ•´é™¤æ•°
                                    t_trimmed = (t // config.n_seraena_frames) * config.n_seraena_frames
                                    x_padded = x_padded[:, :t_trimmed]
                                    t = t_trimmed
                                    logger.debug(f"Trimmed to: {x_padded.shape}")
                                
                                result = x_padded.reshape(n * t//config.n_seraena_frames, config.n_seraena_frames*c, h, w)
                                logger.debug(f"pad_and_group output shape: {result.shape}")
                                return result
                            
                            # ä¿®å¤Seraenaå‚æ•°ç»´åº¦ä¸åŒ¹é…é—®é¢˜
                            # é—®é¢˜ï¼špad_and_groupå†…éƒ¨ä¼špaddingå¸§æ•°ï¼Œä½†ç¬¬ä¸‰ä¸ªå‚æ•°æ²¡æœ‰è€ƒè™‘è¿™ä¸ªpadding
                            
                            actual_frames = frames_target.shape[1]  # åŸå§‹å¸§æ•°ï¼ˆ16ï¼‰
                            frames_to_trim = getattr(model, 'frames_to_trim', 0)  # 3
                            padded_frames = actual_frames + (frames_to_trim if frames_to_trim > 0 else 0)  # 16+3=19
                            
                            # è®¡ç®—åŸºäºpaddingåå¸§æ•°çš„repeat_times
                            repeat_times_padded = padded_frames // config.n_seraena_frames  # 19Ã·1=19
                            
                            # è°ƒè¯•ä¿¡æ¯å·²ç§»é™¤ - è®­ç»ƒæ­£å¸¸åä¸å†éœ€è¦
                            
                            # è®¡ç®—ä¸‰ä¸ªå‚æ•°å¹¶è¾“å‡ºç»´åº¦ä¿¡æ¯
                            param1 = pad_and_group(frames_target)
                            param2 = pad_and_group(decoded)
                            
                            # ä¿®å¤ç¬¬ä¸‰ä¸ªå‚æ•°ï¼šç¡®ä¿ç»´åº¦åŒ¹é…paddingåçš„å¸§æ•°
                            # encoded: [batch, channels, ...] -> éœ€è¦æ‰©å±•åˆ°åŒ¹é…padded_frames Ã— batch
                            batch_size = frames_target.shape[0]
                            target_first_dim = batch_size * repeat_times_padded  # 2Ã—19=38
                            
                            # ä¿®å¤ç¬¬ä¸‰ä¸ªå‚æ•°ï¼šç¡®ä¿å½¢çŠ¶ä¸º4D [N, C, H, W]ï¼ŒåŒ¹é…SeraenaæœŸæœ›
                            # å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡ï¼Œä¿ç•™ç©ºé—´ç»´åº¦: [batch, channels, T, H, W] -> [batch, channels, H, W]
                            encoded_spatial = encoded.mean(dim=2)  # [batch, channels, H, W]
                            
                            # TAEHVä½¿ç”¨4é€šé“latentï¼Œç›´æ¥ä½¿ç”¨ä¸éœ€è¦è°ƒæ•´
                            param3 = encoded_spatial  # [batch, 4, H, W]
                            
                            seraena_target, seraena_debug = seraena.step_and_make_correction_targets(
                                param1,
                                param2,
                                param3
                            )
                            
                            # è½¬å›åŸæ ¼å¼ - é‡æ–°è®¾è®¡ä¸ºä¸pad_and_groupå¯¹ç§°
                            def ungroup_and_unpad(x, original_shape):
                                """
                                å°†pad_and_groupçš„è¾“å‡ºè½¬æ¢å›åŸå§‹æ ¼å¼
                                x: [n_groups, grouped_c, h, w] - pad_and_groupçš„è¾“å‡º
                                original_shape: [N, T, C, H, W] - åŸå§‹å¼ é‡çš„å½¢çŠ¶
                                """
                                n_groups, grouped_c, h, w = x.shape
                                original_n, original_t, original_c, original_h, original_w = original_shape
                                
                                logger.debug(f"ungroup_and_unpad input shape: {x.shape}")
                                logger.debug(f"original_shape: {original_shape}")
                                
                                c = grouped_c // config.n_seraena_frames
                                frames_to_trim = getattr(model, 'frames_to_trim', 0)
                                
                                # è®¡ç®—padåçš„å¸§æ•°
                                padded_frames = original_t + (frames_to_trim if frames_to_trim > 0 else 0)
                                
                                # æ£€æŸ¥æ˜¯å¦è¢«è£å‰ªè¿‡ï¼ˆç¡®ä¿èƒ½è¢«n_seraena_framesæ•´é™¤ï¼‰
                                actual_frames = padded_frames
                                if padded_frames % config.n_seraena_frames != 0:
                                    actual_frames = (padded_frames // config.n_seraena_frames) * config.n_seraena_frames
                                
                                logger.debug(f"padded_frames: {padded_frames}, actual_frames: {actual_frames}")
                                logger.debug(f"Expected n_groups: {original_n * actual_frames // config.n_seraena_frames}")
                                
                                # éªŒè¯ç»´åº¦ä¸€è‡´æ€§
                                expected_n_groups = original_n * actual_frames // config.n_seraena_frames
                                if n_groups != expected_n_groups:
                                    logger.warning(f"Group count mismatch: got {n_groups}, expected {expected_n_groups}")
                                    return None
                                
                                # reshapeå›åŸå§‹æ ¼å¼
                                x_ungrouped = x.reshape(original_n, actual_frames, c, h, w)
                                logger.debug(f"After ungrouping: {x_ungrouped.shape}")
                                
                                # å¦‚æœä¹‹å‰æœ‰paddingï¼Œç°åœ¨éœ€è¦å»æ‰padding
                                if frames_to_trim > 0 and actual_frames > original_t:
                                    # å»æ‰paddingçš„å¸§
                                    trim_amount = min(frames_to_trim, actual_frames - original_t)
                                    x_ungrouped = x_ungrouped[:, :-trim_amount]
                                    logger.debug(f"After removing padding: {x_ungrouped.shape}")
                                
                                return x_ungrouped
                            
                            # ä½¿ç”¨åŸå§‹å¼ é‡çš„å½¢çŠ¶ä¿¡æ¯è¿›è¡Œåå‘è½¬æ¢
                            seraena_target = ungroup_and_unpad(seraena_target, frames_target.shape)
                            if seraena_target is not None:
                                # ç¡®ä¿å½¢çŠ¶åŒ¹é…
                                if seraena_target.shape == decoded.shape:
                                    seraena_loss = F.mse_loss(decoded, seraena_target)
                                    total_loss = total_loss + config.seraena_loss_weight * seraena_loss
                                    logger.debug(f"âœ… Seraena loss computed: {seraena_loss.item():.6f}")
                                else:
                                    logger.warning(f"Shape mismatch: decoded {decoded.shape} vs seraena_target {seraena_target.shape}")
                            else:
                                logger.warning("Skipping Seraena loss due to dimension mismatch")
                            
                        except Exception as e:
                            logger.warning(f"Seraena training failed: {e}, skipping adversarial loss")

                # åå‘ä¼ æ’­ - æ·»åŠ é”™è¯¯æ£€æµ‹å’Œå¤„ç†
                # æ£€æŸ¥lossæ˜¯å¦ä¸ºNaNæˆ–æ— ç©·å¤§
                if not torch.isfinite(total_loss):
                    logger.warning(f"Loss is not finite: {total_loss}, skipping step")
                    optimizer.zero_grad()
                    continue
                
                try:
                    accelerator.backward(total_loss)
                    
                    if accelerator.sync_gradients:
                        # æ¢¯åº¦è£å‰ª - å‚è€ƒstage1ä»£ç çš„å¤„ç†æ–¹å¼
                        if accelerator.distributed_type != DistributedType.DEEPSPEED:
                            # éDeepSpeedæƒ…å†µä¸‹æ‰‹åŠ¨è£å‰ªæ¢¯åº¦
                            grad_norm = accelerator.clip_grad_norm_(model.parameters(), config.max_grad_norm)
                            # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°æ˜¯å¦æœ‰é™ï¼ˆä½¿ç”¨math.isfiniteå¤„ç†Python floatï¼‰
                            if not math.isfinite(grad_norm):
                                logger.warning(f"Gradient norm is not finite: {grad_norm}, skipping step")
                                optimizer.zero_grad()
                                continue
                            
                            # è®°å½•æ¢¯åº¦èŒƒæ•°ç”¨äºè°ƒè¯•
                            if global_step % 50 == 0:
                                logger.info(f"Gradient norm: {grad_norm:.4f}")
                        # else: DeepSpeedä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦è£å‰ª
                            
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        logger.error(f"GPU OOM at step {global_step}, skipping batch")
                        torch.cuda.empty_cache()
                        optimizer.zero_grad()
                        continue
                    else:
                        raise e
                
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            # æ£€æŸ¥accumulateæ¢¯åº¦
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                
                # è¯¦ç»†çš„TensorBoardæ—¥å¿—è®°å½•
                log_dict = {
                    "train/loss": total_loss.detach().item(),
                    "train/reconstruction_loss": recon_loss.item(),
                    "train/learning_rate": lr_scheduler.get_last_lr()[0],
                }
                
                # ç¼–ç å™¨æŸå¤±ï¼ˆå¦‚æœæœ‰å‚è€ƒVAEï¼‰
                if vae_ref is not None and 'encoder_loss' in locals():
                    log_dict["train/encoder_loss"] = encoder_loss.item()
                
                # SeraenaæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if seraena is not None and 'seraena_loss' in locals():
                    log_dict["train/seraena_loss"] = seraena_loss.item()
                
                # æ¢¯åº¦èŒƒæ•°è®°å½•ï¼ˆå¦‚æœè®¡ç®—äº†ï¼‰
                if 'grad_norm' in locals() and math.isfinite(grad_norm):
                    log_dict["train/gradient_norm"] = grad_norm
                
                # GPUå†…å­˜ä½¿ç”¨
                if torch.cuda.is_available():
                    memory_gb = torch.cuda.memory_allocated() / (1024**3)
                    log_dict["train/gpu_memory_gb"] = memory_gb
                
                accelerator.log(log_dict, step=global_step)
                train_loss += total_loss.detach().item()
                
                # å®šæœŸå†…å­˜ç®¡ç†å’Œé€šä¿¡ä¼˜åŒ–ï¼ˆæ¯100æ­¥ï¼‰
                if global_step % 100 == 0:
                    torch.cuda.empty_cache()
                    if hasattr(accelerator.state, 'deepspeed_plugin') and accelerator.state.deepspeed_plugin is not None:
                        import gc
                        gc.collect()
                
                # å†…å­˜ç›‘æ§ï¼ˆæ¯50æ­¥ï¼Œé™é»˜æ‰§è¡Œï¼‰
                if global_step % 50 == 0:
                    if torch.cuda.is_available():
                        memory_allocated = torch.cuda.memory_allocated() / 1024**3
                        memory_reserved = torch.cuda.memory_reserved() / 1024**3
                        # ä»…åœ¨å†…å­˜å¼‚å¸¸æ—¶æ‰“å°ï¼ˆè¶…è¿‡35GB reservedï¼‰
                        if memory_reserved > 35.0:
                            logger.warning(f"Step {global_step}: High GPU memory usage - Reserved: {memory_reserved:.2f}GB")

                # ============================================================
                # éªŒè¯æ­¥éª¤ï¼ˆæ¯validation_stepsæ‰§è¡Œä¸€æ¬¡ï¼‰
                # ============================================================
                if global_step % config.validation_steps == 0 and global_step > 0:
                    logger.info(f"ğŸ” Running validation at step {global_step}")
                    
                    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                    accelerator.wait_for_everyone()
                    
                    # åªåœ¨ä¸»è¿›ç¨‹è¿è¡ŒéªŒè¯ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
                    if accelerator.is_main_process:
                        try:
                            # è®¡ç®—éªŒè¯æŒ‡æ ‡
                            val_metrics = compute_validation_metrics(
                                model=accelerator.unwrap_model(model),
                                val_dataloader=val_dataloader,
                                device=accelerator.device,
                                lpips_fn=lpips_fn,
                                num_samples=config.num_validation_samples,
                                logger=logger,
                                use_amp=(accelerator.mixed_precision != "no")
                            )
                            
                            # è®°å½•åˆ°TensorBoard
                            accelerator.log(val_metrics, step=global_step)
                            
                            # ä¿å­˜åˆ°å†å²è®°å½•
                            validation_history['steps'].append(global_step)
                            validation_history['psnr'].append(val_metrics['val/psnr'])
                            validation_history['ssim'].append(val_metrics['val/ssim'])
                            if 'val/lpips' in val_metrics:
                                validation_history['lpips'].append(val_metrics['val/lpips'])
                            
                            # æ‰“å°éªŒè¯ç»“æœ
                            logger.info(f"âœ… Validation Results (Step {global_step}):")
                            logger.info(f"   PSNR:  {val_metrics['val/psnr']:.2f} Â± {val_metrics['val/psnr_std']:.2f} dB")
                            logger.info(f"   SSIM:  {val_metrics['val/ssim']:.4f}")
                            if 'val/lpips' in val_metrics:
                                logger.info(f"   LPIPS: {val_metrics['val/lpips']:.4f}")
                            
                            # ============================================================
                            # æœ€ä½³æ¨¡å‹ä¿å­˜é€»è¾‘
                            # ============================================================
                            current_psnr = val_metrics['val/psnr']
                            
                            if current_psnr > best_val_psnr:
                                # å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹
                                improvement = current_psnr - best_val_psnr
                                best_val_psnr = current_psnr
                                best_val_step = global_step
                                patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
                                
                                logger.info(f"ğŸ† New Best Model! PSNR improved by {improvement:.2f} dB")
                                logger.info(f"   Best PSNR: {best_val_psnr:.2f} dB at step {best_val_step}")
                                
                                # ä¿å­˜æœ€ä½³æ¨¡å‹
                                best_model_dir = os.path.join(config.output_dir, "best_model")
                                
                                # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹
                                if os.path.exists(best_model_dir):
                                    shutil.rmtree(best_model_dir)
                                    logger.info(f"   Removed old best model")
                                
                                # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
                                os.makedirs(best_model_dir, exist_ok=True)
                                
                                # 1. ä¿å­˜æ¨¡å‹æƒé‡
                                model_to_save = accelerator.unwrap_model(model)
                                torch.save(
                                    model_to_save.state_dict(),
                                    os.path.join(best_model_dir, "model.pth")
                                )
                                
                                # 2. ä¿å­˜å…ƒä¿¡æ¯
                                best_model_info = {
                                    'step': global_step,
                                    'psnr': float(current_psnr),
                                    'ssim': float(val_metrics['val/ssim']),
                                    'train_loss': float(total_loss.detach().item()) if 'total_loss' in locals() else 0.0,
                                    'timestamp': datetime.datetime.now().isoformat(),
                                }
                                if 'val/lpips' in val_metrics:
                                    best_model_info['lpips'] = float(val_metrics['val/lpips'])
                                
                                with open(os.path.join(best_model_dir, "model_info.json"), 'w') as f:
                                    json.dump(best_model_info, f, indent=2)
                                
                                logger.info(f"   âœ… Saved best model to {best_model_dir}")
                                
                            else:
                                # æ²¡æœ‰æ”¹å–„
                                patience_counter += 1
                                logger.info(f"âš ï¸  No improvement for {patience_counter} validation(s)")
                                logger.info(f"   Current PSNR: {current_psnr:.2f} dB")
                                logger.info(f"   Best PSNR: {best_val_psnr:.2f} dB (at step {best_val_step})")
                                
                                # ============================================================
                                # æ—©åœæ£€æŸ¥
                                # ============================================================
                                if patience_counter >= patience_limit:
                                    logger.info(f"ğŸ›‘ Early Stopping Triggered!")
                                    logger.info(f"   No improvement for {patience_limit} validations ({patience_limit * config.validation_steps} steps)")
                                    logger.info(f"   Best model: step {best_val_step}, PSNR {best_val_psnr:.2f} dB")
                                    
                                    # ä¿å­˜æ—©åœä¿¡æ¯
                                    early_stop_info = {
                                        'stopped_at_step': global_step,
                                        'best_step': best_val_step,
                                        'best_psnr': float(best_val_psnr),
                                        'patience_limit': patience_limit,
                                        'reason': 'early_stopping'
                                    }
                                    
                                    with open(os.path.join(config.output_dir, "early_stop_info.json"), 'w') as f:
                                        json.dump(early_stop_info, f, indent=2)
                                    
                                    logger.info("   Exiting training loop...")
                                    # è®¾ç½®æ ‡å¿—ï¼Œå¤–å±‚å¾ªç¯æ£€æŸ¥åé€€å‡º
                                    should_stop_training = True
                            
                            # å®šæœŸä¿å­˜éªŒè¯å†å²
                            if global_step % (config.validation_steps * 5) == 0:
                                validation_history_path = os.path.join(config.output_dir, "validation_history.json")
                                with open(validation_history_path, 'w') as f:
                                    json.dump(validation_history, f, indent=2)
                                logger.info(f"   Saved validation history to {validation_history_path}")
                            
                        except Exception as e:
                            logger.error(f"âŒ Validation failed: {e}")
                            import traceback
                            traceback.print_exc()
                    
                    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼ˆç­‰å¾…ä¸»è¿›ç¨‹å®ŒæˆéªŒè¯ï¼‰
                    accelerator.wait_for_everyone()
                    
                    # æ¸…ç†æ˜¾å­˜
                    torch.cuda.empty_cache()
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
                    if 'should_stop_training' in locals() and should_stop_training:
                        logger.info("Early stopping: breaking out of training loop")
                        break  # é€€å‡ºè®­ç»ƒå¾ªç¯

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % config.checkpointing_steps == 0:
                    # ä¿å­˜å‰çš„å†…å­˜å’Œé€šä¿¡æ¸…ç†
                    torch.cuda.empty_cache()  # æ¸…ç†GPUå†…å­˜
                    if accelerator.is_main_process:
                        logger.info(f"Starting checkpoint save at step {global_step}")
                    
                    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿çŠ¶æ€ä¸€è‡´
                    accelerator.wait_for_everyone()
                    
                    try:
                        # ä¿å­˜æ£€æŸ¥ç‚¹ - accelerator.save_state()å†…éƒ¨å¤„ç†å¤šè¿›ç¨‹åè°ƒ
                        save_path = os.path.join(config.output_dir, f"checkpoint-{global_step}")
                        accelerator.save_state(save_path)
                        if accelerator.is_main_process:
                            logger.info(f"âœ… Successfully saved checkpoint to {save_path}")
                        
                        # ä¿å­˜åé¢å¤–æ¸…ç†
                        torch.cuda.empty_cache()
                        
                    except Exception as e:
                        if accelerator.is_main_process:
                            logger.error(f"âŒ Failed to save checkpoint at step {global_step}: {e}")
                        # ç»§ç»­è®­ç»ƒè€Œä¸æ˜¯å´©æºƒ
                    
                    # å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿checkpointä¿å­˜å®Œæˆ
                    accelerator.wait_for_everyone()
                    
                    # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹
                    if accelerator.is_main_process and config.checkpoints_total_limit is not None:
                        try:
                            checkpoints = os.listdir(config.output_dir)
                            checkpoints = [d for d in checkpoints if d.startswith("checkpoint")]
                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split("-")[1]))

                            # åˆ é™¤æœ€è€çš„æ£€æŸ¥ç‚¹
                            if len(checkpoints) > config.checkpoints_total_limit:
                                num_to_remove = len(checkpoints) - config.checkpoints_total_limit
                                removing_checkpoints = checkpoints[0:num_to_remove]

                                logger.info(
                                    f"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints"
                                )
                                logger.info(f"Removing checkpoints: {', '.join(removing_checkpoints)}")

                                for removing_checkpoint in removing_checkpoints:
                                    removing_checkpoint = os.path.join(config.output_dir, removing_checkpoint)
                                    shutil.rmtree(removing_checkpoint)
                        except Exception as e:
                            logger.warning(f"Failed to cleanup old checkpoints: {e}")
                            # ç»§ç»­è®­ç»ƒï¼Œä¸å› ä¸ºæ¸…ç†å¤±è´¥è€Œä¸­æ–­

                # è®°å½•æ—¥å¿—
                if global_step % config.log_every == 0:
                    progress_bar.set_postfix(loss=total_loss.detach().item())

            logs = {"step_loss": total_loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            progress_bar.set_postfix(**logs)

            if global_step >= config.max_train_steps:
                break

    # åˆ›å»ºpipelineå¹¶ä¿å­˜
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        model_unwrapped = accelerator.unwrap_model(model)
        torch.save(model_unwrapped.state_dict(), os.path.join(config.output_dir, "final_model.pth"))
        logger.info(f"Saved final model to {config.output_dir}/final_model.pth")

    accelerator.end_training()


if __name__ == "__main__":
    main()
