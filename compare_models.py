#!/usr/bin/env python3
"""
TAEHV Model Comparison Framework
æ¯”è¾ƒå¤šä¸ªæ¨¡å‹åœ¨åŒä¸€éªŒè¯é›†ä¸Šçš„æ€§èƒ½è¡¨ç°

åŠŸèƒ½ï¼š
- å¹¶æ’å¯è§†åŒ–å¯¹æ¯”
- è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- æ”¹è¿›ç™¾åˆ†æ¯”åˆ†æ
"""

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
from scipy import stats

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent))

from models.taehv import TAEHV
from training.dataset import MiniDataset


class ModelComparator:
    """æ¨¡å‹å¯¹æ¯”å™¨ - æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    
    def __init__(
        self,
        model_configs: List[Dict],
        config,
        device: str = 'cuda',
        num_samples: int = 5
    ):
        """
        Args:
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {'path': str, 'name': str}
            config: è®­ç»ƒé…ç½®
            device: è®¾å¤‡
            num_samples: è¯„ä¼°æ ·æœ¬æ•°
        """
        self.config = config
        self.device = device
        self.num_samples = num_samples
        
        print("=" * 80)
        print("ğŸ”¬ Model Comparison Framework")
        print("=" * 80)
        print()
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        self.models = []
        self.model_names = []
        
        for model_config in model_configs:
            print(f"ğŸ“¦ Loading model: {model_config['name']}")
            print(f"   Path: {model_config['path']}")
            
            model = self._load_model(model_config['path'])
            model.eval()
            
            self.models.append(model)
            self.model_names.append(model_config['name'])
            
            params = sum(p.numel() for p in model.parameters()) / 1e6
            print(f"   âœ… Loaded ({params:.2f}M parameters)")
            print()
        
        # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„ç»“æœ
        self.results = {name: defaultdict(list) for name in self.model_names}
        self.reconstructions = {name: [] for name in self.model_names}
        self.originals = []
        
    def _load_model(self, model_path: str) -> TAEHV:
        """åŠ è½½æ¨¡å‹"""
        model = TAEHV(
            checkpoint_path=None,
            patch_size=self.config.patch_size,
            latent_channels=self.config.latent_channels,
        ).to(self.device)
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # ç§»é™¤module.å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            new_key = k[7:] if k.startswith('module.') else k
            new_state_dict[new_key] = v
        
        model.load_state_dict(new_state_dict, strict=False)
        return model
    
    def evaluate_all_models(self, dataloader: DataLoader, save_dir: Path):
        """åœ¨åŒä¸€æ•°æ®é›†ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"{'=' * 80}")
        print(f"ğŸ” Evaluating {len(self.models)} models on {self.num_samples} samples...")
        print(f"{'=' * 80}")
        print()
        
        sample_count = 0
        
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Processing batches")):
            if sample_count >= self.num_samples:
                break
            
            if isinstance(batch, dict):
                videos = batch['video'].to(self.device)
            else:
                videos = batch.to(self.device)
            
            # ç¡®ä¿è¾“å…¥åœ¨ [0, 1]
            if videos.min() < 0:
                videos = (videos + 1) / 2
            
            # ä¿å­˜åŸå§‹è§†é¢‘
            self.originals.append(videos.cpu())
            
            # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
            for model, model_name in zip(self.models, self.model_names):
                with torch.no_grad():
                    # ç¼–ç å’Œè§£ç 
                    latents = model.encode_video(videos, parallel=True, show_progress_bar=False)
                    reconstructions = model.decode_video(latents, parallel=True, show_progress_bar=False)
                    
                    # å¯¹é½å¸§æ•°
                    frames_to_trim = model.frames_to_trim
                    if reconstructions.shape[1] < videos.shape[1]:
                        videos_trimmed = videos[:, frames_to_trim:frames_to_trim + reconstructions.shape[1]]
                    else:
                        videos_trimmed = videos
                    
                    # ä¿å­˜é‡å»ºç»“æœ
                    self.reconstructions[model_name].append(reconstructions.cpu())
                    
                    # è®¡ç®—æŒ‡æ ‡
                    metrics = self._compute_metrics(videos_trimmed, reconstructions)
                    
                    # æ”¶é›†ç»“æœ
                    for key, value in metrics.items():
                        if isinstance(value, (int, float)):
                            self.results[model_name][key].append(value)
            
            sample_count += videos.shape[0]
        
        print(f"\nâœ… Evaluation complete!")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self._compute_statistics()
        
        # ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–
        self._generate_comparison_visualizations(save_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_comparison_report(save_dir)
    
    def _compute_metrics(self, original: torch.Tensor, reconstructed: torch.Tensor) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # MSE & MAE
        metrics['mse'] = F.mse_loss(reconstructed, original).item()
        metrics['mae'] = F.l1_loss(reconstructed, original).item()
        
        # PSNR
        mse = metrics['mse']
        metrics['psnr'] = 10 * np.log10(1.0 / (mse + 1e-10))
        
        # SSIM (simplified - using correlation)
        orig_flat = original.flatten(2)
        recon_flat = reconstructed.flatten(2)
        
        orig_mean = orig_flat.mean(dim=2, keepdim=True)
        recon_mean = recon_flat.mean(dim=2, keepdim=True)
        
        orig_std = orig_flat.std(dim=2, keepdim=True)
        recon_std = recon_flat.std(dim=2, keepdim=True)
        
        covariance = ((orig_flat - orig_mean) * (recon_flat - recon_mean)).mean(dim=2)
        
        C1 = 0.01 ** 2
        C2 = 0.03 ** 2
        
        ssim = ((2 * orig_mean * recon_mean + C1) * (2 * covariance + C2)) / \
               ((orig_mean**2 + recon_mean**2 + C1) * (orig_std**2 + recon_std**2 + C2))
        
        metrics['ssim'] = ssim.mean().item()
        
        return metrics
    
    def _compute_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        self.statistics = {}
        
        for model_name in self.model_names:
            self.statistics[model_name] = {}
            
            for metric_name, values in self.results[model_name].items():
                if not values:
                    continue
                
                values_array = np.array(values)
                
                self.statistics[model_name][metric_name] = {
                    'mean': float(np.mean(values_array)),
                    'std': float(np.std(values_array)),
                    'median': float(np.median(values_array)),
                    'min': float(np.min(values_array)),
                    'max': float(np.max(values_array)),
                }
    
    def _generate_comparison_visualizations(self, save_dir: Path):
        """ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–"""
        print(f"\n{'=' * 80}")
        print("ğŸ¨ Generating comparison visualizations...")
        print(f"{'=' * 80}")
        
        # 1. å¹¶æ’é‡å»ºå¯¹æ¯”
        self._plot_side_by_side_comparison(save_dir)
        
        # 2. æŒ‡æ ‡å¯¹æ¯”å›¾
        self._plot_metrics_comparison(save_dir)
        
        # 3. æ”¹è¿›åˆ†æ
        self._plot_improvement_analysis(save_dir)
        
        # 4. ç»¼åˆå¯¹æ¯”å¤§å›¾
        self._plot_comprehensive_comparison(save_dir)
        
        print(f"âœ… Visualizations saved to {save_dir}")
    
    def _plot_side_by_side_comparison(self, save_dir: Path):
        """å¹¶æ’é‡å»ºå¯¹æ¯”"""
        print("   ğŸ“Š Creating side-by-side comparison...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ä¸­é—´å¸§
        original = self.originals[0][0]  # [T, C, H, W]
        mid_frame = original.shape[0] // 2
        
        orig_frame = original[mid_frame].numpy().transpose(1, 2, 0)
        
        n_models = len(self.models)
        fig, axes = plt.subplots(2, n_models + 1, figsize=(5 * (n_models + 1), 10))
        
        # ç¬¬ä¸€è¡Œï¼šåŸå›¾å’Œé‡å»º
        axes[0, 0].imshow(np.clip(orig_frame, 0, 1))
        axes[0, 0].set_title('Original', fontsize=14, fontweight='bold')
        axes[0, 0].axis('off')
        
        for i, model_name in enumerate(self.model_names):
            recon = self.reconstructions[model_name][0][0]  # [T, C, H, W]
            recon_frame = recon[min(mid_frame, recon.shape[0] - 1)].numpy().transpose(1, 2, 0)
            
            axes[0, i + 1].imshow(np.clip(recon_frame, 0, 1))
            axes[0, i + 1].set_title(model_name, fontsize=14, fontweight='bold')
            axes[0, i + 1].axis('off')
        
        # ç¬¬äºŒè¡Œï¼šè¯¯å·®çƒ­å›¾
        axes[1, 0].axis('off')
        
        for i, model_name in enumerate(self.model_names):
            recon = self.reconstructions[model_name][0][0]
            recon_frame = recon[min(mid_frame, recon.shape[0] - 1)].numpy().transpose(1, 2, 0)
            
            error = np.abs(orig_frame - recon_frame).mean(axis=2)
            im = axes[1, i + 1].imshow(error, cmap='hot', vmin=0, vmax=0.2)
            axes[1, i + 1].set_title(f'Error: {model_name}', fontsize=12)
            axes[1, i + 1].axis('off')
            plt.colorbar(im, ax=axes[1, i + 1], fraction=0.046)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'side_by_side_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_metrics_comparison(self, save_dir: Path):
        """æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
        print("   ğŸ“Š Creating metrics comparison chart...")
        
        metrics_to_plot = ['psnr', 'ssim', 'mse', 'mae']
        available_metrics = [m for m in metrics_to_plot if m in self.statistics[self.model_names[0]]]
        
        if not available_metrics:
            return
        
        n_metrics = len(available_metrics)
        fig, axes = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 6))
        
        if n_metrics == 1:
            axes = [axes]
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.model_names)))
        
        for i, metric in enumerate(available_metrics):
            means = [self.statistics[name][metric]['mean'] for name in self.model_names]
            stds = [self.statistics[name][metric]['std'] for name in self.model_names]
            
            x = np.arange(len(self.model_names))
            bars = axes[i].bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.8, edgecolor='black')
            
            axes[i].set_xlabel('Model', fontsize=12, fontweight='bold')
            axes[i].set_ylabel(metric.upper(), fontsize=12, fontweight='bold')
            axes[i].set_title(f'{metric.upper()} Comparison', fontsize=14, fontweight='bold')
            axes[i].set_xticks(x)
            axes[i].set_xticklabels(self.model_names, rotation=15, ha='right')
            axes[i].grid(axis='y', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, (bar, mean) in enumerate(zip(bars, means)):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height,
                           f'{mean:.3f}',
                           ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'metrics_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_improvement_analysis(self, save_dir: Path):
        """æ”¹è¿›åˆ†æï¼ˆç›¸å¯¹äºç¬¬ä¸€ä¸ªæ¨¡å‹ï¼‰"""
        print("   ğŸ“Š Creating improvement analysis...")
        
        if len(self.models) < 2:
            return
        
        baseline_name = self.model_names[0]
        metrics_to_plot = ['psnr', 'ssim', 'mse', 'mae']
        available_metrics = [m for m in metrics_to_plot if m in self.statistics[baseline_name]]
        
        if not available_metrics:
            return
        
        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
        improvements = {}
        for model_name in self.model_names[1:]:
            improvements[model_name] = {}
            for metric in available_metrics:
                baseline_val = self.statistics[baseline_name][metric]['mean']
                current_val = self.statistics[model_name][metric]['mean']
                
                # å¯¹äºMSE/MAEï¼Œé™ä½æ˜¯å¥½çš„
                if metric in ['mse', 'mae']:
                    improvement = (baseline_val - current_val) / baseline_val * 100
                else:
                    improvement = (current_val - baseline_val) / baseline_val * 100
                
                improvements[model_name][metric] = improvement
        
        # ç»˜åˆ¶æ”¹è¿›å›¾
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = np.arange(len(available_metrics))
        width = 0.8 / len(improvements)
        
        colors = plt.cm.Set2(np.linspace(0, 1, len(improvements)))
        
        for i, (model_name, color) in enumerate(zip(improvements.keys(), colors)):
            values = [improvements[model_name][m] for m in available_metrics]
            offset = (i - len(improvements)/2 + 0.5) * width
            bars = ax.bar(x + offset, values, width, label=f'{model_name} vs {baseline_name}',
                         color=color, alpha=0.8, edgecolor='black')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:+.1f}%',
                       ha='center', va='bottom' if val > 0 else 'top',
                       fontsize=10, fontweight='bold')
        
        ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')
        ax.set_ylabel('Improvement (%)', fontsize=12, fontweight='bold')
        ax.set_title(f'Performance Improvement Relative to {baseline_name}',
                    fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels([m.upper() for m in available_metrics])
        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)
        ax.legend(fontsize=10)
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'improvement_analysis.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_comprehensive_comparison(self, save_dir: Path):
        """ç»¼åˆå¯¹æ¯”å¤§å›¾"""
        print("   ğŸ“Š Creating comprehensive comparison...")
        
        fig = plt.figure(figsize=(20, 12))
        gs = gridspec.GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)
        
        # æ ‡é¢˜
        fig.suptitle('Comprehensive Model Comparison', fontsize=20, fontweight='bold', y=0.98)
        
        # 1. é‡å»ºå¯¹æ¯”ï¼ˆç¬¬ä¸€è¡Œï¼‰
        original = self.originals[0][0]
        mid_frame = original.shape[0] // 2
        orig_frame = original[mid_frame].numpy().transpose(1, 2, 0)
        
        n_models = len(self.models)
        for i in range(min(n_models + 1, 3)):
            ax = fig.add_subplot(gs[0, i])
            
            if i == 0:
                ax.imshow(np.clip(orig_frame, 0, 1))
                ax.set_title('Original', fontsize=14, fontweight='bold')
            else:
                model_idx = i - 1
                if model_idx < len(self.model_names):
                    model_name = self.model_names[model_idx]
                    recon = self.reconstructions[model_name][0][0]
                    recon_frame = recon[min(mid_frame, recon.shape[0] - 1)].numpy().transpose(1, 2, 0)
                    ax.imshow(np.clip(recon_frame, 0, 1))
                    ax.set_title(model_name, fontsize=14, fontweight='bold')
            
            ax.axis('off')
        
        # 2. PSNRå¯¹æ¯”ï¼ˆç¬¬äºŒè¡Œå·¦ï¼‰
        ax = fig.add_subplot(gs[1, 0])
        if 'psnr' in self.statistics[self.model_names[0]]:
            means = [self.statistics[name]['psnr']['mean'] for name in self.model_names]
            stds = [self.statistics[name]['psnr']['std'] for name in self.model_names]
            x = np.arange(len(self.model_names))
            ax.bar(x, means, yerr=stds, capsize=5, color='skyblue', alpha=0.8, edgecolor='black')
            ax.set_ylabel('PSNR (dB)', fontsize=12, fontweight='bold')
            ax.set_title('PSNR Comparison', fontsize=12, fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(self.model_names, rotation=15, ha='right')
            ax.grid(axis='y', alpha=0.3)
        
        # 3. SSIMå¯¹æ¯”ï¼ˆç¬¬äºŒè¡Œä¸­ï¼‰
        ax = fig.add_subplot(gs[1, 1])
        if 'ssim' in self.statistics[self.model_names[0]]:
            means = [self.statistics[name]['ssim']['mean'] for name in self.model_names]
            stds = [self.statistics[name]['ssim']['std'] for name in self.model_names]
            x = np.arange(len(self.model_names))
            ax.bar(x, means, yerr=stds, capsize=5, color='lightcoral', alpha=0.8, edgecolor='black')
            ax.set_ylabel('SSIM', fontsize=12, fontweight='bold')
            ax.set_title('SSIM Comparison', fontsize=12, fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(self.model_names, rotation=15, ha='right')
            ax.grid(axis='y', alpha=0.3)
        
        # 4. MSEå¯¹æ¯”ï¼ˆç¬¬äºŒè¡Œå³ï¼‰
        ax = fig.add_subplot(gs[1, 2])
        if 'mse' in self.statistics[self.model_names[0]]:
            means = [self.statistics[name]['mse']['mean'] for name in self.model_names]
            stds = [self.statistics[name]['mse']['std'] for name in self.model_names]
            x = np.arange(len(self.model_names))
            ax.bar(x, means, yerr=stds, capsize=5, color='lightgreen', alpha=0.8, edgecolor='black')
            ax.set_ylabel('MSE', fontsize=12, fontweight='bold')
            ax.set_title('MSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(self.model_names, rotation=15, ha='right')
            ax.grid(axis='y', alpha=0.3)
        
        # 5. æ”¹è¿›è¡¨æ ¼ï¼ˆç¬¬ä¸‰è¡Œï¼‰
        ax = fig.add_subplot(gs[2, :])
        ax.axis('off')
        
        if len(self.models) >= 2:
            baseline_name = self.model_names[0]
            table_data = [['Metric', baseline_name]]
            
            for model_name in self.model_names[1:]:
                table_data[0].append(f'{model_name}\n(vs {baseline_name})')
            
            for metric in ['psnr', 'ssim', 'mse', 'mae']:
                if metric not in self.statistics[baseline_name]:
                    continue
                
                row = [metric.upper()]
                baseline_val = self.statistics[baseline_name][metric]['mean']
                row.append(f'{baseline_val:.4f}')
                
                for model_name in self.model_names[1:]:
                    current_val = self.statistics[model_name][metric]['mean']
                    
                    if metric in ['mse', 'mae']:
                        improvement = (baseline_val - current_val) / baseline_val * 100
                    else:
                        improvement = (current_val - baseline_val) / baseline_val * 100
                    
                    row.append(f'{current_val:.4f}\n({improvement:+.1f}%)')
                
                table_data.append(row)
            
            table = ax.table(cellText=table_data, loc='center', cellLoc='center')
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 2)
            
            # è®¾ç½®è¡¨å¤´æ ·å¼
            for i in range(len(table_data[0])):
                table[(0, i)].set_facecolor('#4CAF50')
                table[(0, i)].set_text_props(weight='bold', color='white')
        
        plt.savefig(save_dir / 'comprehensive_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _generate_comparison_report(self, save_dir: Path):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("   ğŸ“„ Generating comparison report...")
        
        report_path = save_dir / 'COMPARISON_REPORT.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ğŸ”¬ Model Comparison Report\n\n")
            f.write(f"**Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Samples**: {self.num_samples}\n\n")
            
            f.write("---\n\n")
            
            # æ¨¡å‹åˆ—è¡¨
            f.write("## ğŸ“¦ Models Evaluated\n\n")
            for i, name in enumerate(self.model_names):
                f.write(f"{i + 1}. **{name}**\n")
            f.write("\n")
            
            # æŒ‡æ ‡å¯¹æ¯”è¡¨
            f.write("## ğŸ“Š Metrics Comparison\n\n")
            f.write("| Metric | " + " | ".join(self.model_names) + " |\n")
            f.write("|--------|" + "|".join(["--------"] * len(self.model_names)) + "|\n")
            
            for metric in ['psnr', 'ssim', 'mse', 'mae']:
                if metric not in self.statistics[self.model_names[0]]:
                    continue
                
                row = [f"**{metric.upper()}**"]
                for name in self.model_names:
                    stats = self.statistics[name][metric]
                    row.append(f"{stats['mean']:.4f} Â± {stats['std']:.4f}")
                
                f.write("| " + " | ".join(row) + " |\n")
            
            f.write("\n")
            
            # æ”¹è¿›åˆ†æ
            if len(self.models) >= 2:
                f.write("## ğŸ“ˆ Improvement Analysis\n\n")
                baseline_name = self.model_names[0]
                f.write(f"*Baseline: {baseline_name}*\n\n")
                
                for model_name in self.model_names[1:]:
                    f.write(f"### {model_name} vs {baseline_name}\n\n")
                    
                    for metric in ['psnr', 'ssim', 'mse', 'mae']:
                        if metric not in self.statistics[baseline_name]:
                            continue
                        
                        baseline_val = self.statistics[baseline_name][metric]['mean']
                        current_val = self.statistics[model_name][metric]['mean']
                        
                        if metric in ['mse', 'mae']:
                            improvement = (baseline_val - current_val) / baseline_val * 100
                            direction = "â†“" if improvement > 0 else "â†‘"
                        else:
                            improvement = (current_val - baseline_val) / baseline_val * 100
                            direction = "â†‘" if improvement > 0 else "â†“"
                        
                        symbol = "âœ…" if improvement > 0 else "âš ï¸"
                        
                        f.write(f"- **{metric.upper()}**: {current_val:.4f} ")
                        f.write(f"({improvement:+.2f}% {direction}) {symbol}\n")
                    
                    f.write("\n")
            
            # ç»“è®º
            f.write("## ğŸ’¡ Summary\n\n")
            
            if len(self.models) >= 2:
                baseline_name = self.model_names[0]
                trained_name = self.model_names[1]
                
                psnr_imp = ((self.statistics[trained_name]['psnr']['mean'] - 
                           self.statistics[baseline_name]['psnr']['mean']) / 
                           self.statistics[baseline_name]['psnr']['mean'] * 100)
                
                if psnr_imp > 0:
                    f.write(f"âœ… **{trained_name}** shows improvement over **{baseline_name}**\n\n")
                    f.write(f"- PSNR improved by **{psnr_imp:.2f}%**\n")
                else:
                    f.write(f"âš ï¸ **{trained_name}** shows degradation compared to **{baseline_name}**\n\n")
            
            f.write("\n---\n\n")
            f.write("*Generated by Model Comparison Framework*\n")
        
        print(f"âœ… Report saved to: {report_path}")


def main():
    parser = argparse.ArgumentParser(
        description='Compare multiple TAEHV models',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example:
  python compare_models.py \\
      --model1 checkpoints/taecvx.pth \\
      --model1_name "Untrained" \\
      --model2 output/checkpoint-19000-merged/merged_model_final.pth \\
      --model2_name "Trained (19k steps)" \\
      --data_root /data/matrix-project/MiniDataset/5M_validation_set/data \\
      --annotation_file /data/matrix-project/MiniDataset/5M_validation_set/stage1_annotations_500_validation_5.json \\
      --num_samples 5
        """
    )
    
    # Model 1
    parser.add_argument('--model1', type=str, required=True,
                       help='Path to first model checkpoint')
    parser.add_argument('--model1_name', type=str, default='Model 1',
                       help='Name for first model')
    
    # Model 2
    parser.add_argument('--model2', type=str, required=True,
                       help='Path to second model checkpoint')
    parser.add_argument('--model2_name', type=str, default='Model 2',
                       help='Name for second model')
    
    # Optional: Model 3 and more
    parser.add_argument('--model3', type=str,
                       help='Path to third model checkpoint (optional)')
    parser.add_argument('--model3_name', type=str, default='Model 3',
                       help='Name for third model')
    
    # Config and data
    parser.add_argument('--config', type=str,
                       default='training/configs/taehv_config_16gpu_h100.py',
                       help='Config file path')
    parser.add_argument('--data_root', type=str, required=True,
                       help='Data root directory')
    parser.add_argument('--annotation_file', type=str, required=True,
                       help='Annotation file path')
    
    # Evaluation settings
    parser.add_argument('--num_samples', type=int, default=5,
                       help='Number of samples to evaluate')
    parser.add_argument('--batch_size', type=int, default=1,
                       help='Batch size')
    parser.add_argument('--output_dir', type=str, default='evaluation_comparison',
                       help='Output directory')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“‹ Loading config from: {args.config}")
    import importlib.util
    spec = importlib.util.spec_from_file_location("config", Path(args.config))
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    config = config_module.args
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    if not Path(args.annotation_file).exists():
        print(f"\nâŒ Annotation file not found: {args.annotation_file}")
        return
    
    if not Path(args.data_root).exists():
        print(f"\nâŒ Data directory not found: {args.data_root}")
        return
    
    # æ„å»ºæ¨¡å‹é…ç½®åˆ—è¡¨
    model_configs = [
        {'path': args.model1, 'name': args.model1_name},
        {'path': args.model2, 'name': args.model2_name},
    ]
    
    if args.model3:
        model_configs.append({'path': args.model3, 'name': args.model3_name})
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“‚ Loading dataset...")
    dataset = MiniDataset(
        annotation_file=args.annotation_file,
        data_dir=args.data_root,
        patch_hw=max(config.height, config.width),
        n_frames=config.n_frames,
        augmentation=False
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    print(f"âœ… Loaded {len(dataset)} samples")
    
    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = ModelComparator(
        model_configs=model_configs,
        config=config,
        device=args.device,
        num_samples=args.num_samples
    )
    
    # è¿è¡Œå¯¹æ¯”è¯„ä¼°
    output_dir = Path(args.output_dir)
    comparator.evaluate_all_models(dataloader, output_dir)
    
    print(f"\nâœ… All results saved to: {output_dir}")
    print("\n" + "=" * 80)
    print("ğŸ‰ Comparison completed successfully!")
    print("=" * 80)


if __name__ == '__main__':
    main()




